* 
* ==> Audit <==
* |--------------|-----------------------|----------|-------|---------|---------------------|---------------------|
|   Command    |         Args          | Profile  | User  | Version |     Start Time      |      End Time       |
|--------------|-----------------------|----------|-------|---------|---------------------|---------------------|
| stop         |                       | minikube | nihar | v1.32.0 | 15 Apr 24 16:58 IST | 15 Apr 24 16:59 IST |
| start        |                       | minikube | nihar | v1.32.0 | 15 Apr 24 18:31 IST | 15 Apr 24 18:33 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 15 Apr 24 18:45 IST | 15 Apr 24 18:45 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 15 Apr 24 21:50 IST | 15 Apr 24 21:50 IST |
| stop         |                       | minikube | nihar | v1.32.0 | 15 Apr 24 22:05 IST | 15 Apr 24 22:06 IST |
| start        | -p kube -n 3          | kube     | nihar | v1.32.0 | 15 Apr 24 22:11 IST | 15 Apr 24 22:20 IST |
| stop         | kube                  | minikube | nihar | v1.32.0 | 15 Apr 24 22:34 IST | 15 Apr 24 22:34 IST |
| stop         | -p kube               | kube     | nihar | v1.32.0 | 15 Apr 24 22:34 IST | 15 Apr 24 22:35 IST |
| start        | -p kube -n 1          | kube     | nihar | v1.32.0 | 16 Apr 24 09:33 IST |                     |
| stop         | -p kube               | kube     | nihar | v1.32.0 | 16 Apr 24 09:50 IST | 16 Apr 24 09:50 IST |
| delete       | -p kukbe              | kukbe    | nihar | v1.32.0 | 16 Apr 24 09:51 IST | 16 Apr 24 09:51 IST |
| delete       | -p kube               | kube     | nihar | v1.32.0 | 16 Apr 24 09:51 IST | 16 Apr 24 09:51 IST |
| start        | -p kube -n 1          | kube     | nihar | v1.32.0 | 16 Apr 24 09:52 IST | 16 Apr 24 09:53 IST |
| stop         | -p kube               | kube     | nihar | v1.32.0 | 16 Apr 24 10:00 IST | 16 Apr 24 10:00 IST |
| delete       | -p kube               | kube     | nihar | v1.32.0 | 16 Apr 24 10:00 IST | 16 Apr 24 10:00 IST |
| start        | -p kube -n 2          | kube     | nihar | v1.32.0 | 16 Apr 24 10:01 IST | 16 Apr 24 10:05 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 16 Apr 24 11:57 IST | 16 Apr 24 11:57 IST |
| start        | -p app                | app      | nihar | v1.32.0 | 16 Apr 24 12:08 IST | 16 Apr 24 12:12 IST |
| stop         | -p app                | app      | nihar | v1.32.0 | 16 Apr 24 12:28 IST | 16 Apr 24 12:29 IST |
| stop         | -p kube               | kube     | nihar | v1.32.0 | 16 Apr 24 12:34 IST | 16 Apr 24 12:35 IST |
| delete       | -p app help kube      | app      | nihar | v1.32.0 | 16 Apr 24 12:35 IST |                     |
| delete       | -p app                | app      | nihar | v1.32.0 | 16 Apr 24 12:35 IST | 16 Apr 24 12:35 IST |
| delete       | -p help               | help     | nihar | v1.32.0 | 16 Apr 24 12:35 IST | 16 Apr 24 12:35 IST |
| delete       | -p kube               | kube     | nihar | v1.32.0 | 16 Apr 24 12:36 IST | 16 Apr 24 12:36 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 16 Apr 24 12:41 IST | 16 Apr 24 12:41 IST |
| start        |                       | minikube | nihar | v1.32.0 | 16 Apr 24 12:48 IST | 16 Apr 24 12:50 IST |
| addons       | enable metrics-server | minikube | nihar | v1.32.0 | 16 Apr 24 12:52 IST | 16 Apr 24 12:52 IST |
| stop         |                       | minikube | nihar | v1.32.0 | 19 Apr 24 10:24 IST | 19 Apr 24 10:24 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 23 Apr 24 17:16 IST | 23 Apr 24 17:16 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 24 Apr 24 10:47 IST | 24 Apr 24 10:48 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 24 Apr 24 10:48 IST | 24 Apr 24 10:48 IST |
| start        |                       | minikube | nihar | v1.32.0 | 24 Apr 24 17:57 IST | 24 Apr 24 18:01 IST |
| tunnel       |                       | minikube | nihar | v1.32.0 | 24 Apr 24 18:10 IST | 24 Apr 24 18:12 IST |
| stop         |                       | minikube | nihar | v1.32.0 | 24 Apr 24 18:12 IST | 24 Apr 24 18:13 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 24 Apr 24 22:17 IST | 24 Apr 24 22:17 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 25 Apr 24 22:39 IST | 25 Apr 24 22:39 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 12:34 IST | 26 Apr 24 12:34 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 12:53 IST | 26 Apr 24 12:54 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 13:02 IST | 26 Apr 24 13:02 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 13:05 IST | 26 Apr 24 13:05 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 14:24 IST |                     |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 14:26 IST | 26 Apr 24 14:26 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 14:28 IST |                     |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 14:31 IST |                     |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 14:33 IST |                     |
| update-check |                       | minikube | nihar | v1.32.0 | 26 Apr 24 16:29 IST | 26 Apr 24 16:29 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 27 Apr 24 12:52 IST | 27 Apr 24 12:52 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 07 May 24 19:47 IST | 07 May 24 19:47 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 07 May 24 20:20 IST | 07 May 24 20:20 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 08 May 24 09:09 IST | 08 May 24 09:09 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 08 May 24 10:06 IST | 08 May 24 10:06 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 08 May 24 10:59 IST | 08 May 24 10:59 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 08 May 24 16:14 IST | 08 May 24 16:14 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 09 May 24 10:50 IST | 09 May 24 10:50 IST |
| update-check |                       | minikube | nihar | v1.32.0 | 11 May 24 17:58 IST | 11 May 24 17:58 IST |
| start        |                       | minikube | nihar | v1.32.0 | 11 May 24 19:49 IST | 11 May 24 19:51 IST |
| ip           |                       | minikube | nihar | v1.32.0 | 11 May 24 19:51 IST | 11 May 24 19:51 IST |
| image        | load resume:v1        | minikube | nihar | v1.32.0 | 11 May 24 20:10 IST |                     |
| image        | load api:v1           | minikube | nihar | v1.32.0 | 11 May 24 20:11 IST |                     |
| image        | load resume:v1        | minikube | nihar | v1.32.0 | 11 May 24 20:12 IST |                     |
|--------------|-----------------------|----------|-------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/05/11 19:49:39
Running on machine: nihar-HP-ProBook-440-G4
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0511 19:49:39.920161   43788 out.go:296] Setting OutFile to fd 1 ...
I0511 19:49:39.920337   43788 out.go:348] isatty.IsTerminal(1) = true
I0511 19:49:39.920342   43788 out.go:309] Setting ErrFile to fd 2...
I0511 19:49:39.920350   43788 out.go:348] isatty.IsTerminal(2) = true
I0511 19:49:39.920667   43788 root.go:338] Updating PATH: /home/nihar/.minikube/bin
I0511 19:49:40.023950   43788 out.go:303] Setting JSON to false
I0511 19:49:40.056967   43788 start.go:128] hostinfo: {"hostname":"nihar-HP-ProBook-440-G4","uptime":36791,"bootTime":1715400389,"procs":258,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-28-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"811b3d56-5a01-4dad-93e1-1ede1bcf1233"}
I0511 19:49:40.057150   43788 start.go:138] virtualization: kvm host
I0511 19:49:40.126524   43788 out.go:177] üòÑ  minikube v1.32.0 on Ubuntu 22.04
I0511 19:49:40.247389   43788 notify.go:220] Checking for updates...
I0511 19:49:40.274738   43788 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0511 19:49:40.275062   43788 driver.go:378] Setting default libvirt URI to qemu:///system
I0511 19:49:40.384057   43788 docker.go:122] docker version: linux-26.0.0:Docker Engine - Community
I0511 19:49:40.384176   43788 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0511 19:49:40.573822   43788 lock.go:35] WriteFile acquiring /home/nihar/.minikube/last_update_check: {Name:mkfad9803844937081ab8b3e09b9f609c16a7e07 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0511 19:49:40.674362   43788 out.go:177] üéâ  minikube 1.33.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.33.0
I0511 19:49:40.832249   43788 out.go:177] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0511 19:49:41.136910   43788 info.go:266] docker info: {ID:02d9dec4-fda8-49e9-93e7-53edd9cae034 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:28 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:42 SystemTime:2024-05-11 19:49:40.533989731 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-28-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8202252288 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:nihar-HP-ProBook-440-G4 Labels:[] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.25.0]] Warnings:<nil>}}
I0511 19:49:41.137271   43788 docker.go:295] overlay module found
I0511 19:49:41.199795   43788 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0511 19:49:41.249052   43788 start.go:298] selected driver: docker
I0511 19:49:41.249083   43788 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nihar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0511 19:49:41.249725   43788 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0511 19:49:41.250136   43788 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0511 19:49:41.340459   43788 info.go:266] docker info: {ID:02d9dec4-fda8-49e9-93e7-53edd9cae034 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:28 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:42 SystemTime:2024-05-11 19:49:41.330810816 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-28-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8202252288 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:nihar-HP-ProBook-440-G4 Labels:[] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.25.0]] Warnings:<nil>}}
I0511 19:49:41.340993   43788 cni.go:84] Creating CNI manager for ""
I0511 19:49:41.341002   43788 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0511 19:49:41.341007   43788 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nihar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0511 19:49:41.398277   43788 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0511 19:49:41.447648   43788 cache.go:121] Beginning downloading kic base image for docker with docker
I0511 19:49:41.529999   43788 out.go:177] üöú  Pulling base image ...
I0511 19:49:41.579450   43788 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0511 19:49:41.579797   43788 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0511 19:49:41.580000   43788 preload.go:148] Found local preload: /home/nihar/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0511 19:49:41.580027   43788 cache.go:56] Caching tarball of preloaded images
I0511 19:49:41.580613   43788 preload.go:174] Found /home/nihar/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0511 19:49:41.580658   43788 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0511 19:49:41.581111   43788 profile.go:148] Saving config to /home/nihar/.minikube/profiles/minikube/config.json ...
I0511 19:49:41.644318   43788 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0511 19:49:41.644339   43788 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0511 19:49:41.644358   43788 cache.go:194] Successfully downloaded all kic artifacts
I0511 19:49:41.644387   43788 start.go:365] acquiring machines lock for minikube: {Name:mk3efa214e5c68b65ece51ade23a4beee7b71bbb Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0511 19:49:41.644503   43788 start.go:369] acquired machines lock for "minikube" in 98.691¬µs
I0511 19:49:41.644522   43788 start.go:96] Skipping create...Using existing machine configuration
I0511 19:49:41.644527   43788 fix.go:54] fixHost starting: 
I0511 19:49:41.644884   43788 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0511 19:49:41.763561   43788 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0511 19:49:41.763661   43788 fix.go:128] unexpected machine state, will restart: <nil>
I0511 19:49:41.828688   43788 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0511 19:49:41.960636   43788 cli_runner.go:164] Run: docker start minikube
I0511 19:49:45.344621   43788 cli_runner.go:217] Completed: docker start minikube: (3.383950878s)
I0511 19:49:45.344692   43788 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0511 19:49:45.363366   43788 kic.go:430] container "minikube" state is running.
I0511 19:49:45.364013   43788 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0511 19:49:45.384112   43788 profile.go:148] Saving config to /home/nihar/.minikube/profiles/minikube/config.json ...
I0511 19:49:45.384398   43788 machine.go:88] provisioning docker machine ...
I0511 19:49:45.384422   43788 ubuntu.go:169] provisioning hostname "minikube"
I0511 19:49:45.384470   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:45.402830   43788 main.go:141] libmachine: Using SSH client type: native
I0511 19:49:45.405179   43788 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0511 19:49:45.405201   43788 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0511 19:49:45.405905   43788 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:58960->127.0.0.1:32772: read: connection reset by peer
I0511 19:49:48.408620   43788 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:58964->127.0.0.1:32772: read: connection reset by peer
I0511 19:49:53.006999   43788 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0511 19:49:53.007240   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:53.034643   43788 main.go:141] libmachine: Using SSH client type: native
I0511 19:49:53.035008   43788 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0511 19:49:53.035037   43788 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0511 19:49:53.175334   43788 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0511 19:49:53.175439   43788 ubuntu.go:175] set auth options {CertDir:/home/nihar/.minikube CaCertPath:/home/nihar/.minikube/certs/ca.pem CaPrivateKeyPath:/home/nihar/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/nihar/.minikube/machines/server.pem ServerKeyPath:/home/nihar/.minikube/machines/server-key.pem ClientKeyPath:/home/nihar/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/nihar/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/nihar/.minikube}
I0511 19:49:53.175536   43788 ubuntu.go:177] setting up certificates
I0511 19:49:53.175555   43788 provision.go:83] configureAuth start
I0511 19:49:53.175757   43788 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0511 19:49:53.218848   43788 provision.go:138] copyHostCerts
I0511 19:49:53.281072   43788 exec_runner.go:144] found /home/nihar/.minikube/ca.pem, removing ...
I0511 19:49:53.281126   43788 exec_runner.go:203] rm: /home/nihar/.minikube/ca.pem
I0511 19:49:53.281413   43788 exec_runner.go:151] cp: /home/nihar/.minikube/certs/ca.pem --> /home/nihar/.minikube/ca.pem (1074 bytes)
I0511 19:49:53.304601   43788 exec_runner.go:144] found /home/nihar/.minikube/cert.pem, removing ...
I0511 19:49:53.304776   43788 exec_runner.go:203] rm: /home/nihar/.minikube/cert.pem
I0511 19:49:53.304939   43788 exec_runner.go:151] cp: /home/nihar/.minikube/certs/cert.pem --> /home/nihar/.minikube/cert.pem (1119 bytes)
I0511 19:49:53.336598   43788 exec_runner.go:144] found /home/nihar/.minikube/key.pem, removing ...
I0511 19:49:53.336632   43788 exec_runner.go:203] rm: /home/nihar/.minikube/key.pem
I0511 19:49:53.336865   43788 exec_runner.go:151] cp: /home/nihar/.minikube/certs/key.pem --> /home/nihar/.minikube/key.pem (1679 bytes)
I0511 19:49:53.337593   43788 provision.go:112] generating server cert: /home/nihar/.minikube/machines/server.pem ca-key=/home/nihar/.minikube/certs/ca.pem private-key=/home/nihar/.minikube/certs/ca-key.pem org=nihar.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0511 19:49:53.684443   43788 provision.go:172] copyRemoteCerts
I0511 19:49:53.684692   43788 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0511 19:49:53.684841   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:53.742845   43788 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nihar/.minikube/machines/minikube/id_rsa Username:docker}
I0511 19:49:53.899923   43788 ssh_runner.go:362] scp /home/nihar/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0511 19:49:54.655042   43788 ssh_runner.go:362] scp /home/nihar/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0511 19:49:54.835563   43788 ssh_runner.go:362] scp /home/nihar/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0511 19:49:55.001251   43788 provision.go:86] duration metric: configureAuth took 1.825674166s
I0511 19:49:55.344027   43788 ubuntu.go:193] setting minikube options for container-runtime
I0511 19:49:55.344519   43788 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0511 19:49:55.344641   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:55.404399   43788 main.go:141] libmachine: Using SSH client type: native
I0511 19:49:55.405408   43788 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0511 19:49:55.405422   43788 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0511 19:49:55.680512   43788 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0511 19:49:55.680544   43788 ubuntu.go:71] root file system type: overlay
I0511 19:49:55.680945   43788 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0511 19:49:55.681144   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:55.729066   43788 main.go:141] libmachine: Using SSH client type: native
I0511 19:49:55.729504   43788 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0511 19:49:55.729577   43788 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0511 19:49:55.923873   43788 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0511 19:49:55.924089   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:55.944996   43788 main.go:141] libmachine: Using SSH client type: native
I0511 19:49:55.945326   43788 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0511 19:49:55.945338   43788 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0511 19:49:56.197098   43788 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0511 19:49:56.197133   43788 machine.go:91] provisioned docker machine in 10.812715764s
I0511 19:49:56.197161   43788 start.go:300] post-start starting for "minikube" (driver="docker")
I0511 19:49:56.197201   43788 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0511 19:49:56.197468   43788 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0511 19:49:56.197667   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:56.246761   43788 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nihar/.minikube/machines/minikube/id_rsa Username:docker}
I0511 19:49:56.596276   43788 ssh_runner.go:195] Run: cat /etc/os-release
I0511 19:49:56.609968   43788 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0511 19:49:56.610227   43788 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0511 19:49:56.610285   43788 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0511 19:49:56.610325   43788 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0511 19:49:56.610350   43788 filesync.go:126] Scanning /home/nihar/.minikube/addons for local assets ...
I0511 19:49:56.629622   43788 filesync.go:126] Scanning /home/nihar/.minikube/files for local assets ...
I0511 19:49:56.791402   43788 start.go:303] post-start completed in 594.211395ms
I0511 19:49:56.791575   43788 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0511 19:49:56.791792   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:56.820713   43788 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nihar/.minikube/machines/minikube/id_rsa Username:docker}
I0511 19:49:57.004232   43788 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0511 19:49:57.025800   43788 fix.go:56] fixHost completed within 15.381249458s
I0511 19:49:57.025840   43788 start.go:83] releasing machines lock for "minikube", held for 15.381313205s
I0511 19:49:57.026047   43788 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0511 19:49:57.056300   43788 ssh_runner.go:195] Run: cat /version.json
I0511 19:49:57.056340   43788 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0511 19:49:57.056350   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:57.056387   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:49:57.075273   43788 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nihar/.minikube/machines/minikube/id_rsa Username:docker}
I0511 19:49:57.075538   43788 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nihar/.minikube/machines/minikube/id_rsa Username:docker}
I0511 19:49:59.150074   43788 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.093671816s)
I0511 19:49:59.150168   43788 ssh_runner.go:235] Completed: cat /version.json: (2.093810929s)
I0511 19:49:59.150767   43788 ssh_runner.go:195] Run: systemctl --version
I0511 19:49:59.327993   43788 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0511 19:49:59.347350   43788 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0511 19:49:59.484606   43788 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0511 19:49:59.484775   43788 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0511 19:49:59.502755   43788 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0511 19:49:59.502774   43788 start.go:472] detecting cgroup driver to use...
I0511 19:49:59.502799   43788 detect.go:199] detected "systemd" cgroup driver on host os
I0511 19:49:59.502905   43788 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0511 19:49:59.575199   43788 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0511 19:49:59.656742   43788 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0511 19:49:59.701131   43788 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0511 19:49:59.701321   43788 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0511 19:49:59.728918   43788 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0511 19:49:59.742338   43788 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0511 19:49:59.753993   43788 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0511 19:49:59.766336   43788 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0511 19:49:59.777586   43788 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0511 19:49:59.789684   43788 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0511 19:49:59.858644   43788 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0511 19:49:59.890602   43788 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0511 19:50:00.037592   43788 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0511 19:50:00.192852   43788 start.go:472] detecting cgroup driver to use...
I0511 19:50:00.192886   43788 detect.go:199] detected "systemd" cgroup driver on host os
I0511 19:50:00.192935   43788 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0511 19:50:00.208752   43788 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0511 19:50:00.208798   43788 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0511 19:50:00.391439   43788 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0511 19:50:00.449860   43788 ssh_runner.go:195] Run: which cri-dockerd
I0511 19:50:00.454501   43788 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0511 19:50:00.464993   43788 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0511 19:50:00.486431   43788 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0511 19:50:00.704295   43788 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0511 19:50:00.797771   43788 docker.go:560] configuring docker to use "systemd" as cgroup driver...
I0511 19:50:00.797866   43788 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0511 19:50:00.853310   43788 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0511 19:50:01.018734   43788 ssh_runner.go:195] Run: sudo systemctl restart docker
I0511 19:50:08.117262   43788 ssh_runner.go:235] Completed: sudo systemctl restart docker: (7.098462817s)
I0511 19:50:08.117464   43788 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0511 19:50:08.242873   43788 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0511 19:50:08.361474   43788 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0511 19:50:08.486522   43788 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0511 19:50:08.596166   43788 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0511 19:50:08.634548   43788 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0511 19:50:08.767090   43788 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0511 19:50:10.502609   43788 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker: (1.735495475s)
I0511 19:50:10.502626   43788 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0511 19:50:10.502715   43788 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0511 19:50:10.507221   43788 start.go:540] Will wait 60s for crictl version
I0511 19:50:10.507268   43788 ssh_runner.go:195] Run: which crictl
I0511 19:50:10.511316   43788 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0511 19:50:11.612490   43788 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.101144994s)
I0511 19:50:11.612505   43788 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0511 19:50:11.612550   43788 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0511 19:50:12.185533   43788 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0511 19:50:12.332928   43788 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0511 19:50:12.333648   43788 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0511 19:50:12.379603   43788 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0511 19:50:12.384803   43788 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0511 19:50:12.399165   43788 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0511 19:50:12.399209   43788 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0511 19:50:12.422124   43788 docker.go:671] Got preloaded images: -- stdout --
bitnami/wordpress:6.5.2-debian-12-r4
bitnami/mariadb:11.3.2-debian-12-r1
nginx:latest
alpine:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.9-0
alpine:3.14
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5
polinux/stress:latest

-- /stdout --
I0511 19:50:12.422229   43788 docker.go:601] Images already preloaded, skipping extraction
I0511 19:50:12.422278   43788 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0511 19:50:12.445410   43788 docker.go:671] Got preloaded images: -- stdout --
bitnami/wordpress:6.5.2-debian-12-r4
bitnami/mariadb:11.3.2-debian-12-r1
nginx:latest
alpine:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.9-0
alpine:3.14
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5
polinux/stress:latest

-- /stdout --
I0511 19:50:12.445425   43788 cache_images.go:84] Images are preloaded, skipping loading
I0511 19:50:12.445510   43788 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0511 19:50:13.952264   43788 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.506715857s)
I0511 19:50:13.952316   43788 cni.go:84] Creating CNI manager for ""
I0511 19:50:13.952343   43788 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0511 19:50:13.952380   43788 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0511 19:50:13.952455   43788 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0511 19:50:13.952770   43788 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0511 19:50:13.952895   43788 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0511 19:50:13.952967   43788 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0511 19:50:14.018811   43788 binaries.go:44] Found k8s binaries, skipping transfer
I0511 19:50:14.019082   43788 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0511 19:50:14.052111   43788 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0511 19:50:14.080599   43788 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0511 19:50:14.102925   43788 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0511 19:50:14.161857   43788 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0511 19:50:14.178699   43788 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0511 19:50:14.206796   43788 certs.go:56] Setting up /home/nihar/.minikube/profiles/minikube for IP: 192.168.49.2
I0511 19:50:14.206845   43788 certs.go:190] acquiring lock for shared ca certs: {Name:mk3a79b76403f4cf67aff09b61b7ade801654e79 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0511 19:50:14.214921   43788 certs.go:199] skipping minikubeCA CA generation: /home/nihar/.minikube/ca.key
I0511 19:50:14.215161   43788 certs.go:199] skipping proxyClientCA CA generation: /home/nihar/.minikube/proxy-client-ca.key
I0511 19:50:14.215446   43788 certs.go:315] skipping minikube-user signed cert generation: /home/nihar/.minikube/profiles/minikube/client.key
I0511 19:50:14.215601   43788 certs.go:315] skipping minikube signed cert generation: /home/nihar/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0511 19:50:14.215764   43788 certs.go:315] skipping aggregator signed cert generation: /home/nihar/.minikube/profiles/minikube/proxy-client.key
I0511 19:50:14.215944   43788 certs.go:437] found cert: /home/nihar/.minikube/certs/home/nihar/.minikube/certs/ca-key.pem (1675 bytes)
I0511 19:50:14.215981   43788 certs.go:437] found cert: /home/nihar/.minikube/certs/home/nihar/.minikube/certs/ca.pem (1074 bytes)
I0511 19:50:14.216022   43788 certs.go:437] found cert: /home/nihar/.minikube/certs/home/nihar/.minikube/certs/cert.pem (1119 bytes)
I0511 19:50:14.216059   43788 certs.go:437] found cert: /home/nihar/.minikube/certs/home/nihar/.minikube/certs/key.pem (1679 bytes)
I0511 19:50:14.216762   43788 ssh_runner.go:362] scp /home/nihar/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0511 19:50:14.287784   43788 ssh_runner.go:362] scp /home/nihar/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0511 19:50:14.322835   43788 ssh_runner.go:362] scp /home/nihar/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0511 19:50:14.352559   43788 ssh_runner.go:362] scp /home/nihar/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0511 19:50:14.384970   43788 ssh_runner.go:362] scp /home/nihar/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0511 19:50:14.415828   43788 ssh_runner.go:362] scp /home/nihar/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0511 19:50:14.444364   43788 ssh_runner.go:362] scp /home/nihar/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0511 19:50:14.474959   43788 ssh_runner.go:362] scp /home/nihar/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0511 19:50:14.507707   43788 ssh_runner.go:362] scp /home/nihar/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0511 19:50:14.536347   43788 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0511 19:50:14.557999   43788 ssh_runner.go:195] Run: openssl version
I0511 19:50:14.650607   43788 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0511 19:50:14.702057   43788 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0511 19:50:14.707191   43788 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Apr 10 12:19 /usr/share/ca-certificates/minikubeCA.pem
I0511 19:50:14.707264   43788 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0511 19:50:14.717367   43788 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0511 19:50:14.729786   43788 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0511 19:50:14.734059   43788 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0511 19:50:14.765911   43788 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0511 19:50:14.788625   43788 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0511 19:50:14.798588   43788 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0511 19:50:14.806579   43788 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0511 19:50:14.814264   43788 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0511 19:50:14.822618   43788 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nihar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0511 19:50:14.822729   43788 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0511 19:50:14.859535   43788 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0511 19:50:14.870373   43788 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0511 19:50:14.870386   43788 kubeadm.go:636] restartCluster start
I0511 19:50:14.870440   43788 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0511 19:50:14.909576   43788 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0511 19:50:14.943268   43788 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /home/nihar/.kube/config
I0511 19:50:15.098037   43788 kubeconfig.go:146] "minikube" context is missing from /home/nihar/.kube/config - will repair!
I0511 19:50:15.102575   43788 lock.go:35] WriteFile acquiring /home/nihar/.kube/config: {Name:mk8a92bed5f27c598e2082c78fbbed881d829d3d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0511 19:50:15.202805   43788 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0511 19:50:15.259080   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:15.259190   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:15.324753   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:15.324776   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:15.324969   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:15.356654   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:15.857555   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:15.857840   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:15.900633   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:16.358126   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:16.358315   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:16.401114   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:16.857628   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:16.857816   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:16.899408   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:17.357197   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:17.357371   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:17.401199   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:17.857627   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:17.857919   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:17.897662   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:18.357892   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:18.358122   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:18.399754   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:18.857067   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:18.857287   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:18.898624   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:19.357355   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:19.357628   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:19.397045   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:19.857612   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:19.857762   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:19.898367   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:20.356882   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:20.357151   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:20.395939   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:20.857845   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:20.858021   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:20.896526   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:21.357112   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:21.357355   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:21.398117   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:21.857082   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:21.857239   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:21.895303   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:22.357661   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:22.357981   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:22.397867   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:22.857618   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:22.857801   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:22.893606   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:23.357472   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:23.357762   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:23.407222   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:23.857092   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:23.857332   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:23.894166   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:24.357757   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:24.358049   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:24.399510   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:24.857083   43788 api_server.go:166] Checking apiserver status ...
I0511 19:50:24.857325   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0511 19:50:24.894738   43788 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0511 19:50:25.260042   43788 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0511 19:50:25.260082   43788 kubeadm.go:1128] stopping kube-system containers ...
I0511 19:50:25.260315   43788 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0511 19:50:25.346794   43788 docker.go:469] Stopping containers: [67038e2d1eca ac246e661945 9eba6f3c68e6 323a4b6a079a 001a188885fc 4fdfaf6a2e3d 409b623014f6 3e7c3fccf570 8314b58e6ecc a52ff30f719d 2c22a978dea0 098a312badd5 83de5357200f cf6145a6f06e 195dabf0a36c 66c642b4f753 839783ed209f e1fce33c4e80 d6ec76340ed8 5547f917b972 e2a056726716 1f8fcebf58d9 53140e08515d 1b9ff8947103 cfabb444b063 3a61071a5934 3d62c552434c 34f59151322b 61b40f54b947]
I0511 19:50:25.347012   43788 ssh_runner.go:195] Run: docker stop 67038e2d1eca ac246e661945 9eba6f3c68e6 323a4b6a079a 001a188885fc 4fdfaf6a2e3d 409b623014f6 3e7c3fccf570 8314b58e6ecc a52ff30f719d 2c22a978dea0 098a312badd5 83de5357200f cf6145a6f06e 195dabf0a36c 66c642b4f753 839783ed209f e1fce33c4e80 d6ec76340ed8 5547f917b972 e2a056726716 1f8fcebf58d9 53140e08515d 1b9ff8947103 cfabb444b063 3a61071a5934 3d62c552434c 34f59151322b 61b40f54b947
I0511 19:50:25.388249   43788 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0511 19:50:25.403038   43788 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0511 19:50:25.414189   43788 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Apr 10 12:19 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Apr 24 12:29 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Apr 10 12:20 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Apr 24 12:29 /etc/kubernetes/scheduler.conf

I0511 19:50:25.414246   43788 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0511 19:50:25.425497   43788 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0511 19:50:25.460822   43788 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0511 19:50:25.486781   43788 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0511 19:50:25.486832   43788 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0511 19:50:25.497479   43788 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0511 19:50:25.508765   43788 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0511 19:50:25.508823   43788 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0511 19:50:25.520033   43788 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0511 19:50:25.531124   43788 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0511 19:50:25.531135   43788 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0511 19:50:26.803921   43788 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (1.272724893s)
I0511 19:50:26.803977   43788 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0511 19:50:27.864773   43788 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.060776002s)
I0511 19:50:27.864785   43788 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0511 19:50:28.064490   43788 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0511 19:50:28.134498   43788 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0511 19:50:28.196525   43788 api_server.go:52] waiting for apiserver process to appear ...
I0511 19:50:28.196575   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:28.208988   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:28.722419   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:29.221737   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:29.721600   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:30.222484   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:30.722302   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:31.222561   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:31.721835   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:32.221775   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:32.722332   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:33.221443   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:33.721492   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:34.222398   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:34.722137   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:35.221919   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:35.722401   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:36.221910   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:36.722139   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:37.222123   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:37.721572   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:38.221900   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:38.722264   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:39.222303   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:39.721667   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:40.222526   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:40.721525   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:41.221832   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:41.722362   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:42.222396   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:42.721975   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:43.222050   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:43.721530   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:44.222330   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:44.722513   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:45.221837   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:45.722031   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:46.221831   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:46.721734   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:47.222008   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:47.722129   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:48.222496   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:48.721722   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:49.222390   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:49.722276   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:50.222275   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:50.722540   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:51.222295   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:51.721948   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:52.222230   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:52.721867   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:50:52.776679   43788 api_server.go:72] duration metric: took 24.580130339s to wait for apiserver process to appear ...
I0511 19:50:52.776733   43788 api_server.go:88] waiting for apiserver healthz status ...
I0511 19:50:52.776798   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:52.777621   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:52.777688   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:52.778401   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:53.279406   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:53.280362   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:53.778996   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:53.780083   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:54.278865   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:54.279838   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:54.779134   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:54.780485   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:55.278639   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:55.279824   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:55.778884   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:55.779796   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:56.279604   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:56.280642   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:56.779180   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:56.779588   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:57.278755   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:57.279840   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:57.779045   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:57.780148   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:58.279131   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:58.280113   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:58.779129   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:58.780145   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:59.278977   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:59.279822   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:50:59.778980   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:50:59.780031   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:00.278534   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:00.279346   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:00.779858   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:00.781691   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:01.279536   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:01.280540   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:01.778539   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:01.779839   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:02.278819   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:02.279363   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:02.778945   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:02.779305   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:03.279497   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:03.280281   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:03.778673   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:03.779144   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:04.279065   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:04.279460   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:04.779137   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:04.779532   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:05.278525   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:05.279201   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:05.779610   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:05.780784   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:06.278896   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:06.279307   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:06.779128   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:06.779493   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:07.279064   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:07.279953   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:07.779426   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:07.780413   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:08.279130   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:08.279449   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:08.779226   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:08.779598   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:09.279001   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:09.279370   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:09.779247   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:09.780783   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:10.278910   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:10.280037   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:10.779471   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:10.780553   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:11.279059   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:11.279336   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:11.779246   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:11.780377   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:12.279448   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:12.279840   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:12.779232   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:12.779764   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:13.278747   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:13.279873   43788 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0511 19:51:13.779418   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:15.955669   43788 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0511 19:51:15.955690   43788 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0511 19:51:15.955704   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:16.217343   43788 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0511 19:51:16.217412   43788 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0511 19:51:16.278879   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:16.295326   43788 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0511 19:51:16.295396   43788 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0511 19:51:16.779242   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:16.794839   43788 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0511 19:51:16.794913   43788 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0511 19:51:17.279080   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:17.287439   43788 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0511 19:51:17.327136   43788 api_server.go:141] control plane version: v1.28.3
I0511 19:51:17.327156   43788 api_server.go:131] duration metric: took 24.550412528s to wait for apiserver health ...
I0511 19:51:17.327165   43788 cni.go:84] Creating CNI manager for ""
I0511 19:51:17.327193   43788 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0511 19:51:17.505176   43788 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0511 19:51:17.598800   43788 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0511 19:51:17.615683   43788 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0511 19:51:17.651348   43788 system_pods.go:43] waiting for kube-system pods to appear ...
I0511 19:51:17.668219   43788 system_pods.go:59] 8 kube-system pods found
I0511 19:51:17.668248   43788 system_pods.go:61] "coredns-5dd5756b68-sfcq7" [a1c1d424-82a6-4dee-a90d-5976faeddbf7] Running
I0511 19:51:17.668255   43788 system_pods.go:61] "etcd-minikube" [2e345a14-7800-418d-ba62-2ebbf5624faa] Running
I0511 19:51:17.668261   43788 system_pods.go:61] "kube-apiserver-minikube" [0460101b-8dcc-4a90-8864-949e4faa5cbf] Running
I0511 19:51:17.668267   43788 system_pods.go:61] "kube-controller-manager-minikube" [622fadac-ee29-49ba-9391-2218a28d3826] Running
I0511 19:51:17.668271   43788 system_pods.go:61] "kube-proxy-gbjrx" [edf89cfa-034f-46ac-95bc-526f17bacab3] Running
I0511 19:51:17.668277   43788 system_pods.go:61] "kube-scheduler-minikube" [8d6c65a9-521b-4cb7-ad78-d9605c304604] Running
I0511 19:51:17.668283   43788 system_pods.go:61] "metrics-server-7c66d45ddc-7rmws" [f61007d1-1598-4727-abfa-9c9f543aa3f2] Running
I0511 19:51:17.668288   43788 system_pods.go:61] "storage-provisioner" [3403d36e-fb9a-46e7-8897-a3c64e60252d] Running
I0511 19:51:17.668294   43788 system_pods.go:74] duration metric: took 16.936041ms to wait for pod list to return data ...
I0511 19:51:17.668300   43788 node_conditions.go:102] verifying NodePressure condition ...
I0511 19:51:17.708176   43788 node_conditions.go:122] node storage ephemeral capacity is 959786032Ki
I0511 19:51:17.708192   43788 node_conditions.go:123] node cpu capacity is 4
I0511 19:51:17.708219   43788 node_conditions.go:105] duration metric: took 39.914563ms to run NodePressure ...
I0511 19:51:17.708233   43788 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0511 19:51:21.529269   43788 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (3.821017805s)
I0511 19:51:21.529289   43788 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0511 19:51:21.539345   43788 ops.go:34] apiserver oom_adj: -16
I0511 19:51:21.539356   43788 kubeadm.go:640] restartCluster took 1m6.668965358s
I0511 19:51:21.539363   43788 kubeadm.go:406] StartCluster complete in 1m6.716751113s
I0511 19:51:21.539375   43788 settings.go:142] acquiring lock: {Name:mkcdb2e8a8fee0c67361e33ccffd52072ceb2480 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0511 19:51:21.539485   43788 settings.go:150] Updating kubeconfig:  /home/nihar/.kube/config
I0511 19:51:21.542306   43788 lock.go:35] WriteFile acquiring /home/nihar/.kube/config: {Name:mk8a92bed5f27c598e2082c78fbbed881d829d3d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0511 19:51:21.644419   43788 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0511 19:51:21.644668   43788 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0511 19:51:21.644744   43788 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0511 19:51:21.644813   43788 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0511 19:51:21.644844   43788 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0511 19:51:21.644855   43788 addons.go:240] addon storage-provisioner should already be in state true
I0511 19:51:21.644904   43788 host.go:66] Checking if "minikube" exists ...
I0511 19:51:21.644922   43788 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0511 19:51:21.644936   43788 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0511 19:51:21.645225   43788 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0511 19:51:21.645453   43788 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0511 19:51:21.645602   43788 addons.go:69] Setting dashboard=true in profile "minikube"
I0511 19:51:21.645617   43788 addons.go:231] Setting addon dashboard=true in "minikube"
W0511 19:51:21.645628   43788 addons.go:240] addon dashboard should already be in state true
I0511 19:51:21.645689   43788 host.go:66] Checking if "minikube" exists ...
I0511 19:51:21.646440   43788 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0511 19:51:21.647246   43788 addons.go:69] Setting metrics-server=true in profile "minikube"
I0511 19:51:21.647269   43788 addons.go:231] Setting addon metrics-server=true in "minikube"
W0511 19:51:21.647277   43788 addons.go:240] addon metrics-server should already be in state true
I0511 19:51:21.647393   43788 host.go:66] Checking if "minikube" exists ...
I0511 19:51:21.648215   43788 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0511 19:51:21.933559   43788 out.go:177]     ‚ñ™ Using image registry.k8s.io/metrics-server/metrics-server:v0.6.4
I0511 19:51:21.698982   43788 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0511 19:51:21.889980   43788 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0511 19:51:22.202645   43788 addons.go:423] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0511 19:51:22.220229   43788 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
W0511 19:51:22.261937   43788 addons.go:240] addon default-storageclass should already be in state true
I0511 19:51:22.261957   43788 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0511 19:51:22.477658   43788 host.go:66] Checking if "minikube" exists ...
I0511 19:51:22.261977   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0511 19:51:22.477864   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:51:22.261957   43788 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0511 19:51:22.478442   43788 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0511 19:51:23.307952   43788 out.go:177] üîé  Verifying Kubernetes components...
I0511 19:51:22.706452   43788 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0511 19:51:22.732422   43788 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nihar/.minikube/machines/minikube/id_rsa Username:docker}
I0511 19:51:23.091023   43788 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0511 19:51:23.115928   43788 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0511 19:51:23.533853   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0511 19:51:23.533946   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:51:23.534038   43788 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0511 19:51:23.542162   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0511 19:51:23.762235   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0511 19:51:23.762252   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0511 19:51:23.762269   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:51:23.563242   43788 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nihar/.minikube/machines/minikube/id_rsa Username:docker}
I0511 19:51:23.764997   43788 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0511 19:51:23.793018   43788 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nihar/.minikube/machines/minikube/id_rsa Username:docker}
I0511 19:51:23.794845   43788 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nihar/.minikube/machines/minikube/id_rsa Username:docker}
I0511 19:51:23.832423   43788 addons.go:423] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0511 19:51:23.832434   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0511 19:51:23.960203   43788 addons.go:423] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0511 19:51:23.960228   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0511 19:51:23.971916   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0511 19:51:23.971956   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0511 19:51:23.983480   43788 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0511 19:51:23.984258   43788 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0511 19:51:24.081044   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0511 19:51:24.081058   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0511 19:51:24.092771   43788 addons.go:423] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0511 19:51:24.092785   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0511 19:51:24.150910   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0511 19:51:24.150926   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0511 19:51:24.151127   43788 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0511 19:51:24.209048   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0511 19:51:24.209060   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0511 19:51:24.265908   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0511 19:51:24.265933   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0511 19:51:24.317567   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0511 19:51:24.317605   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0511 19:51:24.375539   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0511 19:51:24.375553   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0511 19:51:24.431543   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0511 19:51:24.431557   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0511 19:51:24.494153   43788 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0511 19:51:24.494169   43788 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0511 19:51:24.562973   43788 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0511 19:51:29.917797   43788 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (8.273356791s)
I0511 19:51:29.917883   43788 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0511 19:51:29.917901   43788 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (6.383852816s)
I0511 19:51:29.917920   43788 api_server.go:52] waiting for apiserver process to appear ...
I0511 19:51:29.917966   43788 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0511 19:51:47.826799   43788 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (23.843286707s)
I0511 19:51:47.826858   43788 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (23.842586761s)
I0511 19:51:47.839956   43788 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (23.688794218s)
I0511 19:51:47.839977   43788 addons.go:467] Verifying addon metrics-server=true in "minikube"
I0511 19:51:48.007030   43788 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (23.444024477s)
I0511 19:51:48.007064   43788 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (18.089086086s)
I0511 19:51:48.007075   43788 api_server.go:72] duration metric: took 25.301419771s to wait for apiserver process to appear ...
I0511 19:51:48.007079   43788 api_server.go:88] waiting for apiserver healthz status ...
I0511 19:51:48.007095   43788 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0511 19:51:48.292758   43788 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0511 19:51:48.334359   43788 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0511 19:51:48.497819   43788 api_server.go:141] control plane version: v1.28.3
I0511 19:51:48.621168   43788 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, metrics-server, dashboard
I0511 19:51:48.621170   43788 api_server.go:131] duration metric: took 614.057171ms to wait for apiserver health ...
I0511 19:51:48.621208   43788 system_pods.go:43] waiting for kube-system pods to appear ...
I0511 19:51:48.781451   43788 addons.go:502] enable addons completed in 27.136688229s: enabled=[storage-provisioner default-storageclass metrics-server dashboard]
I0511 19:51:48.645751   43788 system_pods.go:59] 8 kube-system pods found
I0511 19:51:48.781515   43788 system_pods.go:61] "coredns-5dd5756b68-sfcq7" [a1c1d424-82a6-4dee-a90d-5976faeddbf7] Running
I0511 19:51:48.781529   43788 system_pods.go:61] "etcd-minikube" [2e345a14-7800-418d-ba62-2ebbf5624faa] Running
I0511 19:51:48.781541   43788 system_pods.go:61] "kube-apiserver-minikube" [0460101b-8dcc-4a90-8864-949e4faa5cbf] Running
I0511 19:51:48.781554   43788 system_pods.go:61] "kube-controller-manager-minikube" [622fadac-ee29-49ba-9391-2218a28d3826] Running
I0511 19:51:48.781565   43788 system_pods.go:61] "kube-proxy-gbjrx" [edf89cfa-034f-46ac-95bc-526f17bacab3] Running
I0511 19:51:48.781576   43788 system_pods.go:61] "kube-scheduler-minikube" [8d6c65a9-521b-4cb7-ad78-d9605c304604] Running
I0511 19:51:48.781588   43788 system_pods.go:61] "metrics-server-7c66d45ddc-7rmws" [f61007d1-1598-4727-abfa-9c9f543aa3f2] Running
I0511 19:51:48.781599   43788 system_pods.go:61] "storage-provisioner" [3403d36e-fb9a-46e7-8897-a3c64e60252d] Running
I0511 19:51:48.781611   43788 system_pods.go:74] duration metric: took 160.390068ms to wait for pod list to return data ...
I0511 19:51:48.781625   43788 kubeadm.go:581] duration metric: took 26.075965733s to wait for : map[apiserver:true system_pods:true] ...
I0511 19:51:48.781662   43788 node_conditions.go:102] verifying NodePressure condition ...
I0511 19:51:48.792106   43788 node_conditions.go:122] node storage ephemeral capacity is 959786032Ki
I0511 19:51:48.792134   43788 node_conditions.go:123] node cpu capacity is 4
I0511 19:51:48.792162   43788 node_conditions.go:105] duration metric: took 10.476656ms to run NodePressure ...
I0511 19:51:48.792204   43788 start.go:228] waiting for startup goroutines ...
I0511 19:51:48.792225   43788 start.go:233] waiting for cluster config update ...
I0511 19:51:48.792254   43788 start.go:242] writing updated cluster config ...
I0511 19:51:48.793101   43788 ssh_runner.go:195] Run: rm -f paused
I0511 19:51:48.871820   43788 start.go:600] kubectl: 1.29.3, cluster: 1.28.3 (minor skew: 1)
I0511 19:51:49.008491   43788 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* May 11 14:29:22 minikube cri-dockerd[764]: time="2024-05-11T14:29:22Z" level=info msg="Pulling image mongo:jammy: c21b89d414fc: Extracting [=============================>                     ]  137.6MB/232.6MB"
May 11 14:29:32 minikube cri-dockerd[764]: time="2024-05-11T14:29:32Z" level=info msg="Pulling image mongo:jammy: c21b89d414fc: Extracting [=================================================> ]    229MB/232.6MB"
May 11 14:29:42 minikube cri-dockerd[764]: time="2024-05-11T14:29:42Z" level=info msg="Pulling image mongo:jammy: 4138c7eb3b71: Pull complete "
May 11 14:29:44 minikube cri-dockerd[764]: time="2024-05-11T14:29:44Z" level=info msg="Stop pulling image mongo:jammy: Status: Downloaded newer image for mongo:jammy"
May 11 14:29:48 minikube dockerd[509]: time="2024-05-11T14:29:48.533188248Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:29:48 minikube dockerd[509]: time="2024-05-11T14:29:48.533348904Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:29:52 minikube dockerd[509]: time="2024-05-11T14:29:52.807062879Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:29:52 minikube dockerd[509]: time="2024-05-11T14:29:52.807606070Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:30:08 minikube dockerd[509]: time="2024-05-11T14:30:08.197326798Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:30:08 minikube dockerd[509]: time="2024-05-11T14:30:08.197421111Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:30:11 minikube dockerd[509]: time="2024-05-11T14:30:11.936413614Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:30:11 minikube dockerd[509]: time="2024-05-11T14:30:11.936518536Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:30:36 minikube dockerd[509]: time="2024-05-11T14:30:36.642155553Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:30:36 minikube dockerd[509]: time="2024-05-11T14:30:36.642254310Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:30:40 minikube dockerd[509]: time="2024-05-11T14:30:40.068643022Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:30:40 minikube dockerd[509]: time="2024-05-11T14:30:40.068765920Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:30:40 minikube dockerd[509]: time="2024-05-11T14:30:40.380919371Z" level=info msg="ignoring event" container=56280faecb7fdb8e7a896b8e637aa3345a3efc5f14d5bcba3f7f93e9079d71b7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 11 14:30:42 minikube dockerd[509]: time="2024-05-11T14:30:42.445336167Z" level=info msg="ignoring event" container=b602cee6883444f14c8e5d1f499f8f8769acbc957272621afecb3e2fba9f264c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 11 14:30:45 minikube dockerd[509]: time="2024-05-11T14:30:45.099488074Z" level=error msg="Failed to compute size of container rootfs 56280faecb7fdb8e7a896b8e637aa3345a3efc5f14d5bcba3f7f93e9079d71b7: mount does not exist"
May 11 14:30:46 minikube dockerd[509]: time="2024-05-11T14:30:46.725620308Z" level=info msg="ignoring event" container=31091d0acf000608363bd49b2b196810fc41b0395fd93d3be9d856f5bd15ccfd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 11 14:30:51 minikube dockerd[509]: time="2024-05-11T14:30:51.756496655Z" level=info msg="ignoring event" container=b38eaf999405425035f1c50a84673bb4ce327c1e309a44d2e151566f9f4e2e05 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 11 14:32:27 minikube cri-dockerd[764]: time="2024-05-11T14:32:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bb4f5aa19e345dd6c8285b53d0f1ce8994d646ed1908b2d2baf03098471750ec/resolv.conf as [nameserver 10.96.0.10 search app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 11 14:33:10 minikube dockerd[509]: time="2024-05-11T14:33:10.039139905Z" level=info msg="ignoring event" container=c81a9a77b7fe6b641c9a6e5369c1887d443dc0662fb469393b33511c02550a91 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 11 14:33:11 minikube dockerd[509]: time="2024-05-11T14:33:11.022135283Z" level=info msg="ignoring event" container=bb4f5aa19e345dd6c8285b53d0f1ce8994d646ed1908b2d2baf03098471750ec module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 11 14:33:22 minikube cri-dockerd[764]: time="2024-05-11T14:33:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/729b7d34bfdd8c8f157d3ab1f2c75db1214d1b04a1049d948892770d58bbb850/resolv.conf as [nameserver 10.96.0.10 search data.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 11 14:33:30 minikube dockerd[509]: time="2024-05-11T14:33:30.870209884Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:33:30 minikube dockerd[509]: time="2024-05-11T14:33:30.870368666Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:33:36 minikube dockerd[509]: time="2024-05-11T14:33:36.227693252Z" level=info msg="ignoring event" container=b05afa5c6067881b621437839b26e2aa20fa928a84e96bfdaba2d24a051dd761 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 11 14:33:37 minikube dockerd[509]: time="2024-05-11T14:33:37.084825495Z" level=info msg="ignoring event" container=729b7d34bfdd8c8f157d3ab1f2c75db1214d1b04a1049d948892770d58bbb850 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 11 14:33:38 minikube dockerd[509]: time="2024-05-11T14:33:38.525630435Z" level=error msg="Failed to compute size of container rootfs b05afa5c6067881b621437839b26e2aa20fa928a84e96bfdaba2d24a051dd761: mount does not exist"
May 11 14:33:38 minikube cri-dockerd[764]: time="2024-05-11T14:33:38Z" level=error msg="Error response from daemon: No such container: b05afa5c6067881b621437839b26e2aa20fa928a84e96bfdaba2d24a051dd761 Failed to get stats from container b05afa5c6067881b621437839b26e2aa20fa928a84e96bfdaba2d24a051dd761"
May 11 14:33:43 minikube cri-dockerd[764]: time="2024-05-11T14:33:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a8bb206d5c33cb1cbb1b4aa9f78aeed6925aad0d3654eb0a673ca3e4d253849a/resolv.conf as [nameserver 10.96.0.10 search data.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 11 14:34:20 minikube cri-dockerd[764]: time="2024-05-11T14:34:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a1a83b13214155a2b92bd1b20a4547b76494f9c11b9d5d0d491e6876b611ebd9/resolv.conf as [nameserver 10.96.0.10 search app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 11 14:34:24 minikube dockerd[509]: time="2024-05-11T14:34:24.442298179Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:34:24 minikube dockerd[509]: time="2024-05-11T14:34:24.442465049Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:34:38 minikube cri-dockerd[764]: time="2024-05-11T14:34:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3ee874848cf3578f8378acb08f4c67182c4d1f17d778e6d49450e1fec8da9c15/resolv.conf as [nameserver 10.96.0.10 search ui.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 11 14:34:51 minikube dockerd[509]: time="2024-05-11T14:34:51.015783911Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:34:51 minikube dockerd[509]: time="2024-05-11T14:34:51.015873479Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:34:54 minikube dockerd[509]: time="2024-05-11T14:34:54.606393198Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:34:54 minikube dockerd[509]: time="2024-05-11T14:34:54.606499723Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:35:09 minikube dockerd[509]: time="2024-05-11T14:35:09.754797152Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:35:09 minikube dockerd[509]: time="2024-05-11T14:35:09.754941920Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:35:21 minikube dockerd[509]: time="2024-05-11T14:35:21.743509648Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:35:21 minikube dockerd[509]: time="2024-05-11T14:35:21.743722958Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:35:38 minikube dockerd[509]: time="2024-05-11T14:35:38.953016427Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:35:38 minikube dockerd[509]: time="2024-05-11T14:35:38.953127416Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:36:11 minikube dockerd[509]: time="2024-05-11T14:36:11.723843859Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:36:11 minikube dockerd[509]: time="2024-05-11T14:36:11.723957791Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:36:33 minikube dockerd[509]: time="2024-05-11T14:36:33.737881836Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:36:33 minikube dockerd[509]: time="2024-05-11T14:36:33.738015036Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:37:44 minikube dockerd[509]: time="2024-05-11T14:37:44.828428456Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:37:44 minikube dockerd[509]: time="2024-05-11T14:37:44.828976043Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:38:00 minikube dockerd[509]: time="2024-05-11T14:38:00.661012938Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:38:00 minikube dockerd[509]: time="2024-05-11T14:38:00.661188618Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:38:46 minikube dockerd[509]: time="2024-05-11T14:38:46.777609691Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:38:46 minikube dockerd[509]: time="2024-05-11T14:38:46.777661994Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:40:36 minikube dockerd[509]: time="2024-05-11T14:40:36.955759149Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:40:36 minikube dockerd[509]: time="2024-05-11T14:40:36.955884785Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 11 14:40:53 minikube dockerd[509]: time="2024-05-11T14:40:53.829614661Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 11 14:40:53 minikube dockerd[509]: time="2024-05-11T14:40:53.829669648Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
0a53e7fc08c15       ff65a94ec485e       9 minutes ago       Running             mongo                       0                   a8bb206d5c33c       mongo-7d44888d6f-7x54p
90ab0497f40cb       07655ddf2eebe       20 minutes ago      Running             kubernetes-dashboard        14                  40462ff47f32c       kubernetes-dashboard-8694d4445c-cpj6d
8d9a535f1110f       6e38f40d628db       20 minutes ago      Running             storage-provisioner         19                  07b600e6c2f03       storage-provisioner
c259d093f741f       a608c686bac93       20 minutes ago      Running             metrics-server              4                   ae9fa37dfc72d       metrics-server-7c66d45ddc-7rmws
d77f55f9f72f4       115053965e86b       21 minutes ago      Running             dashboard-metrics-scraper   8                   3ee11054ab315       dashboard-metrics-scraper-7fd5cb4ddc-8ffqt
8c9db876abcae       07655ddf2eebe       21 minutes ago      Exited              kubernetes-dashboard        13                  40462ff47f32c       kubernetes-dashboard-8694d4445c-cpj6d
ef7f2f129d106       ead0a4a53df89       21 minutes ago      Running             coredns                     8                   efc35d82c9008       coredns-5dd5756b68-sfcq7
ca070e12ad22e       a608c686bac93       21 minutes ago      Exited              metrics-server              3                   ae9fa37dfc72d       metrics-server-7c66d45ddc-7rmws
06f55425f45ce       6e38f40d628db       21 minutes ago      Exited              storage-provisioner         18                  07b600e6c2f03       storage-provisioner
ee7119a6df24f       bfc896cf80fba       21 minutes ago      Running             kube-proxy                  8                   ae2d10d30e2d2       kube-proxy-gbjrx
ae7f875ad7081       6d1b4fd1b182d       22 minutes ago      Running             kube-scheduler              8                   31931f7b46ca4       kube-scheduler-minikube
2b01219b47824       10baa1ca17068       22 minutes ago      Running             kube-controller-manager     10                  e527744d6db5f       kube-controller-manager-minikube
308eedac8ab50       73deb9a3f7025       22 minutes ago      Running             etcd                        8                   a5feb3845ae8a       etcd-minikube
0b5586d895bfc       5374347291230       22 minutes ago      Running             kube-apiserver              8                   f7303d41d3c84       kube-apiserver-minikube
0933338fec399       115053965e86b       2 weeks ago         Exited              dashboard-metrics-scraper   7                   2ac7263a5d7e7       dashboard-metrics-scraper-7fd5cb4ddc-8ffqt
323a4b6a079ab       ead0a4a53df89       2 weeks ago         Exited              coredns                     7                   3e7c3fccf570f       coredns-5dd5756b68-sfcq7
4fdfaf6a2e3d9       bfc896cf80fba       2 weeks ago         Exited              kube-proxy                  7                   8314b58e6ecc5       kube-proxy-gbjrx
2c22a978dea01       10baa1ca17068       2 weeks ago         Exited              kube-controller-manager     9                   839783ed209fd       kube-controller-manager-minikube
098a312badd51       5374347291230       2 weeks ago         Exited              kube-apiserver              7                   66c642b4f753f       kube-apiserver-minikube
83de5357200f4       73deb9a3f7025       2 weeks ago         Exited              etcd                        7                   d6ec76340ed8e       etcd-minikube
cf6145a6f06e9       6d1b4fd1b182d       2 weeks ago         Exited              kube-scheduler              7                   e1fce33c4e808       kube-scheduler-minikube

* 
* ==> coredns [323a4b6a079a] <==
* [INFO] 10.244.0.48:48619 - 61004 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000603867s
[INFO] 10.244.0.48:48619 - 42328 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000685279s
[INFO] 10.244.0.48:50483 - 44585 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000098393s
[INFO] 10.244.0.48:47630 - 58663 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000057251s
[INFO] 10.244.0.48:47630 - 44585 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000246757s
[INFO] 10.244.0.48:50483 - 32807 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.00129764s
[INFO] 10.244.0.48:47138 - 15391 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000292511s
[INFO] 10.244.0.48:41375 - 47347 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000424034s
[INFO] 10.244.0.48:41375 - 42986 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000589463s
[INFO] 10.244.0.48:47138 - 48231 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000966641s
[INFO] 10.244.0.48:51574 - 44810 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000580367s
[INFO] 10.244.0.48:51574 - 39539 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000768089s
[INFO] 10.244.0.48:51893 - 15825 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000325542s
[INFO] 10.244.0.48:51893 - 55783 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000469598s
[INFO] 10.244.0.48:54204 - 22232 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000554013s
[INFO] 10.244.0.48:54204 - 48593 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000640157s
[INFO] 10.244.0.48:55443 - 37357 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000104002s
[INFO] 10.244.0.48:55443 - 39657 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000047539s
[INFO] 10.244.0.48:60648 - 46294 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000572139s
[INFO] 10.244.0.48:60648 - 11043 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000822453s
[INFO] 10.244.0.48:44338 - 27661 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000331664s
[INFO] 10.244.0.48:44338 - 33560 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000379368s
[INFO] 10.244.0.48:49302 - 25534 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000217163s
[INFO] 10.244.0.48:49302 - 37302 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000355247s
[INFO] 10.244.0.48:53716 - 7862 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000721964s
[INFO] 10.244.0.48:53716 - 14782 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.001005884s
[INFO] 10.244.0.48:51700 - 10893 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000396461s
[INFO] 10.244.0.48:51700 - 52612 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000600492s
[INFO] 10.244.0.48:36606 - 9509 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.001147008s
[INFO] 10.244.0.48:36606 - 41258 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.001822149s
[INFO] 10.244.0.48:59796 - 49262 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000238326s
[INFO] 10.244.0.48:58533 - 17277 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000382088s
[INFO] 10.244.0.48:59796 - 45152 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000512259s
[INFO] 10.244.0.48:58533 - 3958 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000780223s
[INFO] 10.244.0.48:49263 - 33091 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000251081s
[INFO] 10.244.0.48:45840 - 55895 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000474214s
[INFO] 10.244.0.48:49263 - 54103 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000670991s
[INFO] 10.244.0.48:45840 - 44867 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.001054233s
[INFO] 10.244.0.48:45827 - 5965 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.000439809s
[INFO] 10.244.0.48:45827 - 36212 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.000721465s
[INFO] 10.244.0.48:57934 - 62580 "A IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.000861634s
[INFO] 10.244.0.48:35968 - 7347 "AAAA IN happy-panda-mariadb.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00038454s
[INFO] 10.244.0.48:35968 - 40357 "A IN happy-panda-mariadb.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000550867s
[INFO] 10.244.0.48:49402 - 54370 "AAAA IN happy-panda-mariadb.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000324761s
[INFO] 10.244.0.48:49402 - 15981 "A IN happy-panda-mariadb.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000563635s
[INFO] 10.244.0.48:41116 - 10985 "A IN happy-panda-mariadb. udp 37 false 512" SERVFAIL qr,rd 37 0.002271372s
[INFO] 10.244.0.48:41116 - 49633 "AAAA IN happy-panda-mariadb. udp 37 false 512" SERVFAIL qr,rd 37 0.002437602s
[INFO] 10.244.0.48:41116 - 49633 "AAAA IN happy-panda-mariadb. udp 37 false 512" SERVFAIL qr,aa,rd 37 0.000198952s
[INFO] 10.244.0.48:41116 - 10985 "A IN happy-panda-mariadb. udp 37 false 512" SERVFAIL qr,aa,rd 37 0.000390373s
[INFO] 10.244.0.48:57934 - 17229 "AAAA IN happy-panda-mariadb.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.111367662s
[INFO] 10.244.0.48:52622 - 65197 "AAAA IN happy-panda-mariadb.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000219944s
[INFO] 10.244.0.48:52622 - 4248 "A IN happy-panda-mariadb.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000352956s
[INFO] 10.244.0.48:55385 - 52126 "AAAA IN happy-panda-mariadb.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000198096s
[INFO] 10.244.0.48:55385 - 46982 "A IN happy-panda-mariadb.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000354619s
[INFO] 10.244.0.48:35075 - 26203 "AAAA IN happy-panda-mariadb. udp 37 false 512" SERVFAIL qr,aa,rd 37 0.000198061s
[INFO] 10.244.0.48:35075 - 25445 "A IN happy-panda-mariadb. udp 37 false 512" SERVFAIL qr,aa,rd 37 0.000356971s
[INFO] 10.244.0.48:35075 - 26203 "AAAA IN happy-panda-mariadb. udp 37 false 512" SERVFAIL qr,aa,rd 37 0.000181807s
[INFO] 10.244.0.48:35075 - 25445 "A IN happy-panda-mariadb. udp 37 false 512" SERVFAIL qr,aa,rd 37 0.000341369s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [ef7f2f129d10] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:49665 - 11011 "HINFO IN 2610350652964823193.8767710784713372536. udp 57 false 512" NXDOMAIN qr,rd,ra 132 5.357800665s
[INFO] 127.0.0.1:44521 - 23611 "HINFO IN 2610350652964823193.8767710784713372536. udp 57 false 512" NXDOMAIN qr,rd,ra 132 2.3564397550000002s
[INFO] 127.0.0.1:49562 - 37384 "HINFO IN 2610350652964823193.8767710784713372536. udp 57 false 512" NXDOMAIN qr,aa,rd,ra 132 0.000197255s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_04_10T17_50_28_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 10 Apr 2024 12:19:37 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 11 May 2024 14:43:25 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 11 May 2024 14:40:21 +0000   Wed, 10 Apr 2024 12:19:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 11 May 2024 14:40:21 +0000   Wed, 10 Apr 2024 12:19:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 11 May 2024 14:40:21 +0000   Wed, 10 Apr 2024 12:19:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 11 May 2024 14:40:21 +0000   Wed, 10 Apr 2024 12:19:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  959786032Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8010012Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  959786032Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8010012Ki
  pods:               110
System Info:
  Machine ID:                 19863bd7babb49c4ad8c7867d67f9bce
  System UUID:                40bd4bf8-44cd-4086-a511-3eee17865134
  Boot ID:                    02f70de0-b5e4-45f5-89ff-bd05c1054286
  Kernel Version:             6.5.0-28-generic
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  app                         api-7665c9f64f-4psjb                          500m (12%!)(MISSING)    500m (12%!)(MISSING)  128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     9m17s
  data                        mongo-7d44888d6f-7x54p                        500m (12%!)(MISSING)    500m (12%!)(MISSING)  128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     9m53s
  exp                         ping-google                                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 coredns-5dd5756b68-sfcq7                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     31d
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         31d
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31d
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31d
  kube-system                 kube-proxy-gbjrx                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31d
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31d
  kube-system                 metrics-server-7c66d45ddc-7rmws               100m (2%!)(MISSING)     0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         25d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-8ffqt    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-cpj6d         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31d
  ui                          resume-66d5fbff68-8srpc                       500m (12%!)(MISSING)    500m (12%!)(MISSING)  128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     8m59s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                2350m (58%!)(MISSING)  1500m (37%!)(MISSING)
  memory             754Mi (9%!)(MISSING)   554Mi (7%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 17d                kube-proxy       
  Normal  Starting                 20m                kube-proxy       
  Normal  Starting                 17d                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  17d                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasNoDiskPressure    17d (x8 over 17d)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17d (x7 over 17d)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientMemory  17d (x8 over 17d)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  RegisteredNode           17d                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 23m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  23m (x8 over 23m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    23m (x8 over 23m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     23m (x7 over 23m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  23m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           22m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.000055] ACPI: thermal: [Firmware Bug]: No valid trip found
[  +0.008904] hpet_acpi_add: no address or irqs in _CRS
[  +0.032030] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000139] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.819876] r8169 0000:02:00.0: can't disable ASPM; OS doesn't have ASPM control
[  +0.015094] usb: port power management may be unreliable
[  +5.217222] systemd[1]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.446436] block sda: the capability attribute has been deprecated.
[ +10.547071] hp_wmi: query 0x4 returned error 0x5
[  +1.152291] ACPI Warning: \_SB.PCI0.RP01.PXSX._DSM: Argument #4 type mismatch - Found [Buffer], ACPI requires [Package] (20230331/nsarguments-61)
[  +0.104262] nouveau 0000:01:00.0: bus: MMIO read of 00000000 FAULT at 6013d4 [ PRIVRING ]
[  +1.297424] nouveau 0000:01:00.0: [drm] No compatible format found
[  +2.733198] snd_hda_codec_conexant hdaudioC0D0: vmaster hook already present before cdev!
[  +0.325893] Bluetooth: hci0: Malformed MSFT vendor event: 0x02
[  +0.077811] Bluetooth: hci0: Reading supported features failed (-16)
[  +0.000097] Bluetooth: hci0: HCI LE Coded PHY feature bit is set, but its usage is not supported.
[  +0.047449] thermal thermal_zone7: failed to read out thermal zone (-61)
[ +19.329410] xhci_hcd 0000:00:14.0: xHC error in resume, USBSTS 0x411, Reinit
[  +1.011972] done.
[  +0.071469] thermal thermal_zone7: failed to read out thermal zone (-61)
[  +1.440140] Bluetooth: hci0: Malformed MSFT vendor event: 0x02
[  +0.005925] Bluetooth: hci0: Reading supported features failed (-16)
[  +0.000015] Bluetooth: hci0: HCI LE Coded PHY feature bit is set, but its usage is not supported.
[  +5.691327] kauditd_printk_skb: 31 callbacks suppressed
[May11 07:03] atkbd serio0: Unknown key pressed (translated set 2, code 0x85 on isa0060/serio0).
[  +0.000024] atkbd serio0: Use 'setkeycodes e005 <keycode>' to make it known.
[ +25.836926] nouveau 0000:01:00.0: bus: MMIO read of 00000000 FAULT at 6013d4 [ PRIVRING ]
[ +12.873133] workqueue: pm_runtime_work hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +9.844015] nouveau 0000:01:00.0: bus: MMIO read of 00000000 FAULT at 6013d4 [ PRIVRING ]
[May11 07:04] hid-generic 0005:3434:0280.0003: unknown main item tag 0x0
[  +8.720714] nouveau 0000:01:00.0: bus: MMIO read of 00000000 FAULT at 6013d4 [ PRIVRING ]
[ +12.288532] nouveau 0000:01:00.0: bus: MMIO read of 00000000 FAULT at 6013d4 [ PRIVRING ]
[May11 07:33] nouveau 0000:01:00.0: bus: MMIO read of 00000000 FAULT at 6013d4 [ PRIVRING ]
[  +2.124475] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211
[  +7.012732] workqueue: delayed_fput hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +3.284050] workqueue: pm_runtime_work hogged CPU for >10000us 8 times, consider switching to WQ_UNBOUND
[May11 07:36] workqueue: delayed_fput hogged CPU for >10000us 8 times, consider switching to WQ_UNBOUND
[May11 07:48] workqueue: delayed_fput hogged CPU for >10000us 16 times, consider switching to WQ_UNBOUND
[May11 09:01] workqueue: delayed_fput hogged CPU for >10000us 32 times, consider switching to WQ_UNBOUND
[May11 09:19] workqueue: delayed_fput hogged CPU for >10000us 64 times, consider switching to WQ_UNBOUND
[May11 09:27] workqueue: pm_runtime_work hogged CPU for >10000us 16 times, consider switching to WQ_UNBOUND
[May11 10:50] done.
[  +0.097078] thermal thermal_zone7: failed to read out thermal zone (-61)
[  +2.804039] Bluetooth: hci0: Malformed MSFT vendor event: 0x02
[  +0.327076] Bluetooth: hci0: Reading supported features failed (-16)
[  +0.000028] Bluetooth: hci0: HCI LE Coded PHY feature bit is set, but its usage is not supported.
[  +3.932030] atkbd serio0: Unknown key pressed (translated set 2, code 0x85 on isa0060/serio0).
[  +0.000020] atkbd serio0: Use 'setkeycodes e005 <keycode>' to make it known.
[  +8.275203] hid-generic 0005:3434:0280.0004: unknown main item tag 0x0
[May11 10:57] workqueue: delayed_fput hogged CPU for >10000us 128 times, consider switching to WQ_UNBOUND
[May11 11:47] workqueue: delayed_fput hogged CPU for >10000us 256 times, consider switching to WQ_UNBOUND
[May11 14:14] workqueue: delayed_fput hogged CPU for >10000us 512 times, consider switching to WQ_UNBOUND

* 
* ==> etcd [308eedac8ab5] <==
* {"level":"info","ts":"2024-05-11T14:42:57.934557Z","caller":"traceutil/trace.go:171","msg":"trace[1820682097] linearizableReadLoop","detail":"{readStateIndex:60700; appliedIndex:60698; }","duration":"160.469304ms","start":"2024-05-11T14:42:57.774042Z","end":"2024-05-11T14:42:57.934511Z","steps":["trace[1820682097] 'read index received'  (duration: 20.542554ms)","trace[1820682097] 'applied index is now lower than readState.Index'  (duration: 139.923945ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-11T14:42:57.934952Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"160.9425ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-11T14:42:57.935071Z","caller":"traceutil/trace.go:171","msg":"trace[1990667178] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:48307; }","duration":"161.076428ms","start":"2024-05-11T14:42:57.773959Z","end":"2024-05-11T14:42:57.935035Z","steps":["trace[1990667178] 'agreement among raft nodes before linearized reading'  (duration: 160.71003ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:42:57.93545Z","caller":"traceutil/trace.go:171","msg":"trace[645434671] transaction","detail":"{read_only:false; response_revision:48307; number_of_response:1; }","duration":"407.946416ms","start":"2024-05-11T14:42:57.527463Z","end":"2024-05-11T14:42:57.935409Z","steps":["trace[645434671] 'process raft request'  (duration: 406.770235ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:42:57.935459Z","caller":"traceutil/trace.go:171","msg":"trace[557794963] transaction","detail":"{read_only:false; response_revision:48306; number_of_response:1; }","duration":"411.921925ms","start":"2024-05-11T14:42:57.523478Z","end":"2024-05-11T14:42:57.9354Z","steps":["trace[557794963] 'process raft request'  (duration: 271.000791ms)","trace[557794963] 'compare'  (duration: 138.744624ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-11T14:42:57.935791Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-11T14:42:57.527427Z","time spent":"408.123482ms","remote":"127.0.0.1:45720","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:48305 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-05-11T14:42:57.935819Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-11T14:42:57.523432Z","time spent":"412.157361ms","remote":"127.0.0.1:45604","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:48299 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128029104461672661 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2024-05-11T14:42:58.208372Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"132.020458ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-11T14:42:58.208446Z","caller":"traceutil/trace.go:171","msg":"trace[1501045207] range","detail":"{range_begin:/registry/controllers/; range_end:/registry/controllers0; response_count:0; response_revision:48307; }","duration":"132.106775ms","start":"2024-05-11T14:42:58.076326Z","end":"2024-05-11T14:42:58.208432Z","steps":["trace[1501045207] 'count revisions from in-memory index tree'  (duration: 131.782853ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:43:02.254088Z","caller":"traceutil/trace.go:171","msg":"trace[452340449] linearizableReadLoop","detail":"{readStateIndex:60703; appliedIndex:60702; }","duration":"124.796882ms","start":"2024-05-11T14:43:02.129239Z","end":"2024-05-11T14:43:02.254036Z","steps":["trace[452340449] 'read index received'  (duration: 124.299761ms)","trace[452340449] 'applied index is now lower than readState.Index'  (duration: 494.284¬µs)"],"step_count":2}
{"level":"info","ts":"2024-05-11T14:43:02.254475Z","caller":"traceutil/trace.go:171","msg":"trace[134723540] transaction","detail":"{read_only:false; response_revision:48310; number_of_response:1; }","duration":"263.056646ms","start":"2024-05-11T14:43:01.991174Z","end":"2024-05-11T14:43:02.254231Z","steps":["trace[134723540] 'process raft request'  (duration: 262.559971ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:02.254683Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"125.460795ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-05-11T14:43:02.254842Z","caller":"traceutil/trace.go:171","msg":"trace[2085648495] range","detail":"{range_begin:/registry/daemonsets/; range_end:/registry/daemonsets0; response_count:0; response_revision:48310; }","duration":"125.647477ms","start":"2024-05-11T14:43:02.129156Z","end":"2024-05-11T14:43:02.254804Z","steps":["trace[2085648495] 'agreement among raft nodes before linearized reading'  (duration: 125.115858ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:43:03.300427Z","caller":"traceutil/trace.go:171","msg":"trace[1063818919] linearizableReadLoop","detail":"{readStateIndex:60704; appliedIndex:60703; }","duration":"294.687488ms","start":"2024-05-11T14:43:03.00568Z","end":"2024-05-11T14:43:03.300368Z","steps":["trace[1063818919] 'read index received'  (duration: 294.21212ms)","trace[1063818919] 'applied index is now lower than readState.Index'  (duration: 471.787¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-05-11T14:43:03.300906Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"251.149142ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:890"}
{"level":"info","ts":"2024-05-11T14:43:03.301073Z","caller":"traceutil/trace.go:171","msg":"trace[941696597] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:48310; }","duration":"251.336143ms","start":"2024-05-11T14:43:03.049696Z","end":"2024-05-11T14:43:03.301032Z","steps":["trace[941696597] 'agreement among raft nodes before linearized reading'  (duration: 251.014015ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:03.301036Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"295.3897ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:9"}
{"level":"info","ts":"2024-05-11T14:43:03.301695Z","caller":"traceutil/trace.go:171","msg":"trace[1488040589] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:48310; }","duration":"296.069348ms","start":"2024-05-11T14:43:03.005587Z","end":"2024-05-11T14:43:03.301656Z","steps":["trace[1488040589] 'agreement among raft nodes before linearized reading'  (duration: 295.080823ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:03.995231Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"316.488586ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-11T14:43:03.997332Z","caller":"traceutil/trace.go:171","msg":"trace[1171696421] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:48310; }","duration":"318.643352ms","start":"2024-05-11T14:43:03.678622Z","end":"2024-05-11T14:43:03.997265Z","steps":["trace[1171696421] 'range keys from in-memory index tree'  (duration: 316.398736ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:43:04.785009Z","caller":"traceutil/trace.go:171","msg":"trace[1582802206] transaction","detail":"{read_only:false; response_revision:48311; number_of_response:1; }","duration":"513.928036ms","start":"2024-05-11T14:43:04.271033Z","end":"2024-05-11T14:43:04.784961Z","steps":["trace[1582802206] 'process raft request'  (duration: 513.662158ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:04.785277Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-11T14:43:04.270997Z","time spent":"514.136198ms","remote":"127.0.0.1:45720","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:48310 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-05-11T14:43:05.323969Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"322.180346ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-11T14:43:05.324188Z","caller":"traceutil/trace.go:171","msg":"trace[2123249212] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:48311; }","duration":"322.425864ms","start":"2024-05-11T14:43:05.001723Z","end":"2024-05-11T14:43:05.324149Z","steps":["trace[2123249212] 'range keys from in-memory index tree'  (duration: 321.951369ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:05.324322Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-11T14:43:05.001692Z","time spent":"322.591867ms","remote":"127.0.0.1:45580","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-05-11T14:43:05.324399Z","caller":"traceutil/trace.go:171","msg":"trace[841118529] transaction","detail":"{read_only:false; response_revision:48312; number_of_response:1; }","duration":"300.324466ms","start":"2024-05-11T14:43:05.024025Z","end":"2024-05-11T14:43:05.324349Z","steps":["trace[841118529] 'process raft request'  (duration: 299.463907ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:05.324722Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-11T14:43:05.02398Z","time spent":"300.53836ms","remote":"127.0.0.1:49498","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:48304 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2024-05-11T14:43:07.016667Z","caller":"traceutil/trace.go:171","msg":"trace[1846341772] transaction","detail":"{read_only:false; response_revision:48313; number_of_response:1; }","duration":"221.328161ms","start":"2024-05-11T14:43:06.795324Z","end":"2024-05-11T14:43:07.016652Z","steps":["trace[1846341772] 'process raft request'  (duration: 221.234995ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:07.987508Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"287.467642ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029104461672705 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:48306 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128029104461672703 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2024-05-11T14:43:07.987921Z","caller":"traceutil/trace.go:171","msg":"trace[1563591137] transaction","detail":"{read_only:false; response_revision:48314; number_of_response:1; }","duration":"336.928565ms","start":"2024-05-11T14:43:07.650945Z","end":"2024-05-11T14:43:07.987874Z","steps":["trace[1563591137] 'process raft request'  (duration: 48.95013ms)","trace[1563591137] 'compare'  (duration: 287.284628ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-11T14:43:07.988165Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-11T14:43:07.650907Z","time spent":"337.119542ms","remote":"127.0.0.1:45604","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:48306 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128029104461672703 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2024-05-11T14:43:07.987793Z","caller":"traceutil/trace.go:171","msg":"trace[585385177] linearizableReadLoop","detail":"{readStateIndex:60709; appliedIndex:60708; }","duration":"216.389425ms","start":"2024-05-11T14:43:07.771332Z","end":"2024-05-11T14:43:07.987721Z","steps":["trace[585385177] 'read index received'  (duration: 60.537¬µs)","trace[585385177] 'applied index is now lower than readState.Index'  (duration: 216.325065ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-11T14:43:07.988722Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"217.383386ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-11T14:43:07.988959Z","caller":"traceutil/trace.go:171","msg":"trace[1052419174] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:48314; }","duration":"217.670127ms","start":"2024-05-11T14:43:07.771258Z","end":"2024-05-11T14:43:07.988928Z","steps":["trace[1052419174] 'agreement among raft nodes before linearized reading'  (duration: 217.130473ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:43:11.307927Z","caller":"traceutil/trace.go:171","msg":"trace[172820178] transaction","detail":"{read_only:false; response_revision:48317; number_of_response:1; }","duration":"196.67196ms","start":"2024-05-11T14:43:11.111194Z","end":"2024-05-11T14:43:11.307866Z","steps":["trace[172820178] 'process raft request'  (duration: 196.384028ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:43:17.584251Z","caller":"traceutil/trace.go:171","msg":"trace[454120756] linearizableReadLoop","detail":"{readStateIndex:60717; appliedIndex:60716; }","duration":"128.760028ms","start":"2024-05-11T14:43:17.455439Z","end":"2024-05-11T14:43:17.584199Z","steps":["trace[454120756] 'read index received'  (duration: 128.475264ms)","trace[454120756] 'applied index is now lower than readState.Index'  (duration: 281.791¬µs)"],"step_count":2}
{"level":"info","ts":"2024-05-11T14:43:17.584355Z","caller":"traceutil/trace.go:171","msg":"trace[443980076] transaction","detail":"{read_only:false; response_revision:48321; number_of_response:1; }","duration":"179.414261ms","start":"2024-05-11T14:43:17.404893Z","end":"2024-05-11T14:43:17.584307Z","steps":["trace[443980076] 'process raft request'  (duration: 179.041464ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:17.58464Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"129.221658ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:136"}
{"level":"info","ts":"2024-05-11T14:43:17.584778Z","caller":"traceutil/trace.go:171","msg":"trace[665369128] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:48321; }","duration":"129.386687ms","start":"2024-05-11T14:43:17.455362Z","end":"2024-05-11T14:43:17.584749Z","steps":["trace[665369128] 'agreement among raft nodes before linearized reading'  (duration: 129.143177ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:43:22.147355Z","caller":"traceutil/trace.go:171","msg":"trace[213610371] linearizableReadLoop","detail":"{readStateIndex:60722; appliedIndex:60721; }","duration":"372.933388ms","start":"2024-05-11T14:43:21.774378Z","end":"2024-05-11T14:43:22.147312Z","steps":["trace[213610371] 'read index received'  (duration: 372.738136ms)","trace[213610371] 'applied index is now lower than readState.Index'  (duration: 192.759¬µs)"],"step_count":2}
{"level":"info","ts":"2024-05-11T14:43:22.147555Z","caller":"traceutil/trace.go:171","msg":"trace[590157598] transaction","detail":"{read_only:false; response_revision:48325; number_of_response:1; }","duration":"494.251502ms","start":"2024-05-11T14:43:21.653236Z","end":"2024-05-11T14:43:22.147488Z","steps":["trace[590157598] 'process raft request'  (duration: 493.787112ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:22.148371Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-11T14:43:21.653199Z","time spent":"495.040257ms","remote":"127.0.0.1:45720","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:48323 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-05-11T14:43:22.147868Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"373.550405ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-11T14:43:22.149305Z","caller":"traceutil/trace.go:171","msg":"trace[1985890516] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:48325; }","duration":"374.949753ms","start":"2024-05-11T14:43:21.774264Z","end":"2024-05-11T14:43:22.149214Z","steps":["trace[1985890516] 'agreement among raft nodes before linearized reading'  (duration: 373.421931ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:22.149649Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-11T14:43:21.774228Z","time spent":"375.308866ms","remote":"127.0.0.1:45580","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-05-11T14:43:25.847611Z","caller":"traceutil/trace.go:171","msg":"trace[1941130437] transaction","detail":"{read_only:false; response_revision:48327; number_of_response:1; }","duration":"167.457784ms","start":"2024-05-11T14:43:25.680098Z","end":"2024-05-11T14:43:25.847555Z","steps":["trace[1941130437] 'process raft request'  (duration: 166.622927ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:43:26.455284Z","caller":"traceutil/trace.go:171","msg":"trace[301181586] transaction","detail":"{read_only:false; response_revision:48328; number_of_response:1; }","duration":"212.778906ms","start":"2024-05-11T14:43:26.242457Z","end":"2024-05-11T14:43:26.455236Z","steps":["trace[301181586] 'process raft request'  (duration: 212.401055ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:27.792477Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"224.992133ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029104461672778 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:48322 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128029104461672776 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2024-05-11T14:43:27.792821Z","caller":"traceutil/trace.go:171","msg":"trace[2121830034] transaction","detail":"{read_only:false; response_revision:48329; number_of_response:1; }","duration":"291.957018ms","start":"2024-05-11T14:43:27.500809Z","end":"2024-05-11T14:43:27.792766Z","steps":["trace[2121830034] 'process raft request'  (duration: 66.524016ms)","trace[2121830034] 'compare'  (duration: 224.774792ms)"],"step_count":2}
{"level":"info","ts":"2024-05-11T14:43:28.759504Z","caller":"traceutil/trace.go:171","msg":"trace[1548762853] transaction","detail":"{read_only:false; response_revision:48330; number_of_response:1; }","duration":"296.352878ms","start":"2024-05-11T14:43:28.463119Z","end":"2024-05-11T14:43:28.759472Z","steps":["trace[1548762853] 'process raft request'  (duration: 296.232271ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:43:31.085628Z","caller":"traceutil/trace.go:171","msg":"trace[1181909251] transaction","detail":"{read_only:false; response_revision:48332; number_of_response:1; }","duration":"313.327665ms","start":"2024-05-11T14:43:30.772247Z","end":"2024-05-11T14:43:31.085575Z","steps":["trace[1181909251] 'process raft request'  (duration: 313.05015ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:31.085944Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-11T14:43:30.772228Z","time spent":"313.536458ms","remote":"127.0.0.1:45720","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:48330 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-05-11T14:43:33.273077Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"181.299281ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2024-05-11T14:43:33.273307Z","caller":"traceutil/trace.go:171","msg":"trace[1512594876] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:48332; }","duration":"181.537072ms","start":"2024-05-11T14:43:33.091723Z","end":"2024-05-11T14:43:33.27326Z","steps":["trace[1512594876] 'range keys from in-memory index tree'  (duration: 181.149619ms)"],"step_count":1}
{"level":"info","ts":"2024-05-11T14:43:33.576567Z","caller":"traceutil/trace.go:171","msg":"trace[2049210010] linearizableReadLoop","detail":"{readStateIndex:60733; appliedIndex:60732; }","duration":"265.310478ms","start":"2024-05-11T14:43:33.311197Z","end":"2024-05-11T14:43:33.576507Z","steps":["trace[2049210010] 'read index received'  (duration: 265.013329ms)","trace[2049210010] 'applied index is now lower than readState.Index'  (duration: 294.6¬µs)"],"step_count":2}
{"level":"info","ts":"2024-05-11T14:43:33.576964Z","caller":"traceutil/trace.go:171","msg":"trace[161704905] transaction","detail":"{read_only:false; response_revision:48333; number_of_response:1; }","duration":"295.125802ms","start":"2024-05-11T14:43:33.281544Z","end":"2024-05-11T14:43:33.576669Z","steps":["trace[161704905] 'process raft request'  (duration: 294.699786ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:33.577126Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"265.895723ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:890"}
{"level":"info","ts":"2024-05-11T14:43:33.57729Z","caller":"traceutil/trace.go:171","msg":"trace[973495169] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:48333; }","duration":"266.152445ms","start":"2024-05-11T14:43:33.3111Z","end":"2024-05-11T14:43:33.577252Z","steps":["trace[973495169] 'agreement among raft nodes before linearized reading'  (duration: 265.632936ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-11T14:43:33.907511Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"229.790754ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-11T14:43:33.90786Z","caller":"traceutil/trace.go:171","msg":"trace[1643301612] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:48333; }","duration":"230.157512ms","start":"2024-05-11T14:43:33.677637Z","end":"2024-05-11T14:43:33.907794Z","steps":["trace[1643301612] 'range keys from in-memory index tree'  (duration: 229.703768ms)"],"step_count":1}

* 
* ==> etcd [83de5357200f] <==
* {"level":"warn","ts":"2024-04-24T12:42:47.146403Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.406706ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028726741255593 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/serviceaccounts/default/happy-panda-wordpress\" mod_revision:46090 > success:<request_delete_range:<key:\"/registry/serviceaccounts/default/happy-panda-wordpress\" > > failure:<request_range:<key:\"/registry/serviceaccounts/default/happy-panda-wordpress\" > >>","response":"size:20"}
{"level":"info","ts":"2024-04-24T12:42:47.146601Z","caller":"traceutil/trace.go:171","msg":"trace[907610106] transaction","detail":"{read_only:false; number_of_response:1; response_revision:46760; }","duration":"267.991968ms","start":"2024-04-24T12:42:46.878572Z","end":"2024-04-24T12:42:47.146564Z","steps":["trace[907610106] 'process raft request'  (duration: 154.262859ms)","trace[907610106] 'compare'  (duration: 113.058327ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-24T12:42:47.478279Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"183.584842ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028726741255594 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/serviceaccounts/default/happy-panda-mariadb\" mod_revision:46091 > success:<request_delete_range:<key:\"/registry/serviceaccounts/default/happy-panda-mariadb\" > > failure:<request_range:<key:\"/registry/serviceaccounts/default/happy-panda-mariadb\" > >>","response":"size:20"}
{"level":"info","ts":"2024-04-24T12:42:47.478741Z","caller":"traceutil/trace.go:171","msg":"trace[188132583] linearizableReadLoop","detail":"{readStateIndex:58855; appliedIndex:58853; }","duration":"592.323509ms","start":"2024-04-24T12:42:46.886325Z","end":"2024-04-24T12:42:47.478649Z","steps":["trace[188132583] 'read index received'  (duration: 146.594146ms)","trace[188132583] 'applied index is now lower than readState.Index'  (duration: 445.726488ms)"],"step_count":2}
{"level":"info","ts":"2024-04-24T12:42:47.478765Z","caller":"traceutil/trace.go:171","msg":"trace[1450484127] transaction","detail":"{read_only:false; number_of_response:1; response_revision:46761; }","duration":"599.84122ms","start":"2024-04-24T12:42:46.878878Z","end":"2024-04-24T12:42:47.478719Z","steps":["trace[1450484127] 'process raft request'  (duration: 415.568311ms)","trace[1450484127] 'compare'  (duration: 182.793543ms)"],"step_count":2}
{"level":"info","ts":"2024-04-24T12:42:47.478954Z","caller":"traceutil/trace.go:171","msg":"trace[61781762] transaction","detail":"{read_only:false; response_revision:46762; number_of_response:1; }","duration":"591.195454ms","start":"2024-04-24T12:42:46.887732Z","end":"2024-04-24T12:42:47.478927Z","steps":["trace[61781762] 'process raft request'  (duration: 590.74707ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-24T12:42:47.479049Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-24T12:42:46.878796Z","time spent":"600.107152ms","remote":"127.0.0.1:33572","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":57,"response count":0,"response size":44,"request content":"compare:<target:MOD key:\"/registry/serviceaccounts/default/happy-panda-mariadb\" mod_revision:46091 > success:<request_delete_range:<key:\"/registry/serviceaccounts/default/happy-panda-mariadb\" > > failure:<request_range:<key:\"/registry/serviceaccounts/default/happy-panda-mariadb\" > >"}
{"level":"warn","ts":"2024-04-24T12:42:47.479106Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-24T12:42:46.887694Z","time spent":"591.330395ms","remote":"127.0.0.1:33438","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":760,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/events/default/happy-panda-wordpress-66789d9f64-j87dr.17c937dfa5428055\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/happy-panda-wordpress-66789d9f64-j87dr.17c937dfa5428055\" value_size:662 lease:8128028726741255555 >> failure:<>"}
{"level":"warn","ts":"2024-04-24T12:42:47.479885Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"593.594338ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/happy-panda-wordpress-66789d9f64-j87dr\" ","response":"range_response_count:1 size:11421"}
{"level":"info","ts":"2024-04-24T12:42:47.480015Z","caller":"traceutil/trace.go:171","msg":"trace[851351636] range","detail":"{range_begin:/registry/pods/default/happy-panda-wordpress-66789d9f64-j87dr; range_end:; response_count:1; response_revision:46762; }","duration":"593.729166ms","start":"2024-04-24T12:42:46.886253Z","end":"2024-04-24T12:42:47.479982Z","steps":["trace[851351636] 'agreement among raft nodes before linearized reading'  (duration: 593.313548ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-24T12:42:47.48007Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"547.318902ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-04-24T12:42:47.480124Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-24T12:42:46.88622Z","time spent":"593.874557ms","remote":"127.0.0.1:33552","response type":"/etcdserverpb.KV/Range","request count":0,"request size":63,"response count":1,"response size":11445,"request content":"key:\"/registry/pods/default/happy-panda-wordpress-66789d9f64-j87dr\" "}
{"level":"info","ts":"2024-04-24T12:42:47.480214Z","caller":"traceutil/trace.go:171","msg":"trace[1939994715] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:46762; }","duration":"547.458498ms","start":"2024-04-24T12:42:46.932715Z","end":"2024-04-24T12:42:47.480173Z","steps":["trace[1939994715] 'agreement among raft nodes before linearized reading'  (duration: 547.254966ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-24T12:42:47.480342Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-24T12:42:46.932684Z","time spent":"547.622476ms","remote":"127.0.0.1:33342","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-04-24T12:42:47.480863Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"494.028682ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:890"}
{"level":"info","ts":"2024-04-24T12:42:47.480972Z","caller":"traceutil/trace.go:171","msg":"trace[951721233] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:46762; }","duration":"494.1368ms","start":"2024-04-24T12:42:46.986802Z","end":"2024-04-24T12:42:47.480939Z","steps":["trace[951721233] 'agreement among raft nodes before linearized reading'  (duration: 493.938078ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-24T12:42:47.481082Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-24T12:42:46.986768Z","time spent":"494.280632ms","remote":"127.0.0.1:33524","response type":"/etcdserverpb.KV/Range","request count":0,"request size":77,"response count":1,"response size":914,"request content":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" "}
{"level":"info","ts":"2024-04-24T12:42:48.352313Z","caller":"traceutil/trace.go:171","msg":"trace[244937349] transaction","detail":"{read_only:false; response_revision:46766; number_of_response:1; }","duration":"127.308649ms","start":"2024-04-24T12:42:48.224961Z","end":"2024-04-24T12:42:48.352269Z","steps":["trace[244937349] 'process raft request'  (duration: 127.054473ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-24T12:42:48.655282Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.675659ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourcequotas/\" range_end:\"/registry/resourcequotas0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-24T12:42:48.655425Z","caller":"traceutil/trace.go:171","msg":"trace[1150545167] range","detail":"{range_begin:/registry/resourcequotas/; range_end:/registry/resourcequotas0; response_count:0; response_revision:46766; }","duration":"100.851336ms","start":"2024-04-24T12:42:48.55454Z","end":"2024-04-24T12:42:48.655391Z","steps":["trace[1150545167] 'count revisions from in-memory index tree'  (duration: 100.47984ms)"],"step_count":1}
{"level":"info","ts":"2024-04-24T12:42:51.009106Z","caller":"traceutil/trace.go:171","msg":"trace[1825261558] transaction","detail":"{read_only:false; response_revision:46768; number_of_response:1; }","duration":"283.895554ms","start":"2024-04-24T12:42:50.725158Z","end":"2024-04-24T12:42:51.009054Z","steps":["trace[1825261558] 'process raft request'  (duration: 283.480533ms)"],"step_count":1}
{"level":"info","ts":"2024-04-24T12:42:51.595484Z","caller":"traceutil/trace.go:171","msg":"trace[1438476585] linearizableReadLoop","detail":"{readStateIndex:58865; appliedIndex:58864; }","duration":"166.079251ms","start":"2024-04-24T12:42:51.429357Z","end":"2024-04-24T12:42:51.595436Z","steps":["trace[1438476585] 'read index received'  (duration: 80.18853ms)","trace[1438476585] 'applied index is now lower than readState.Index'  (duration: 85.887831ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-24T12:42:51.595853Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"166.570908ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/happy-panda-mariadb-0\" ","response":"range_response_count:1 size:9153"}
{"level":"info","ts":"2024-04-24T12:42:51.595966Z","caller":"traceutil/trace.go:171","msg":"trace[1475406397] range","detail":"{range_begin:/registry/pods/default/happy-panda-mariadb-0; range_end:; response_count:1; response_revision:46770; }","duration":"166.706751ms","start":"2024-04-24T12:42:51.429229Z","end":"2024-04-24T12:42:51.595936Z","steps":["trace[1475406397] 'agreement among raft nodes before linearized reading'  (duration: 166.380864ms)"],"step_count":1}
{"level":"info","ts":"2024-04-24T12:42:51.595993Z","caller":"traceutil/trace.go:171","msg":"trace[1662708376] transaction","detail":"{read_only:false; response_revision:46770; number_of_response:1; }","duration":"335.379371ms","start":"2024-04-24T12:42:51.260559Z","end":"2024-04-24T12:42:51.595939Z","steps":["trace[1662708376] 'process raft request'  (duration: 249.112195ms)","trace[1662708376] 'compare'  (duration: 85.410417ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-24T12:42:51.596256Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-24T12:42:51.260512Z","time spent":"335.587761ms","remote":"127.0.0.1:33388","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:46731 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128028726741255623 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2024-04-24T12:42:51.9775Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"163.488837ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/happy-panda-mariadb-0\" ","response":"range_response_count:1 size:9153"}
{"level":"info","ts":"2024-04-24T12:42:51.977664Z","caller":"traceutil/trace.go:171","msg":"trace[1470715958] range","detail":"{range_begin:/registry/pods/default/happy-panda-mariadb-0; range_end:; response_count:1; response_revision:46770; }","duration":"163.664959ms","start":"2024-04-24T12:42:51.813963Z","end":"2024-04-24T12:42:51.977628Z","steps":["trace[1470715958] 'range keys from in-memory index tree'  (duration: 163.172094ms)"],"step_count":1}
{"level":"info","ts":"2024-04-24T12:42:52.131171Z","caller":"traceutil/trace.go:171","msg":"trace[413281311] transaction","detail":"{read_only:false; response_revision:46771; number_of_response:1; }","duration":"149.812111ms","start":"2024-04-24T12:42:51.981272Z","end":"2024-04-24T12:42:52.131084Z","steps":["trace[413281311] 'process raft request'  (duration: 149.535418ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-24T12:42:52.646879Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"116.260681ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028726741255636 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/default/happy-panda-wordpress-66789d9f64-j87dr\" mod_revision:46759 > success:<request_put:<key:\"/registry/pods/default/happy-panda-wordpress-66789d9f64-j87dr\" value_size:11411 >> failure:<request_range:<key:\"/registry/pods/default/happy-panda-wordpress-66789d9f64-j87dr\" > >>","response":"size:18"}
{"level":"info","ts":"2024-04-24T12:42:52.647324Z","caller":"traceutil/trace.go:171","msg":"trace[1902501287] linearizableReadLoop","detail":"{readStateIndex:58869; appliedIndex:58868; }","duration":"186.386732ms","start":"2024-04-24T12:42:52.460898Z","end":"2024-04-24T12:42:52.647285Z","steps":["trace[1902501287] 'read index received'  (duration: 69.567408ms)","trace[1902501287] 'applied index is now lower than readState.Index'  (duration: 116.815014ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-24T12:42:52.647632Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"186.773032ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2024-04-24T12:42:52.647918Z","caller":"traceutil/trace.go:171","msg":"trace[1842841288] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:46773; }","duration":"187.060039ms","start":"2024-04-24T12:42:52.46082Z","end":"2024-04-24T12:42:52.64788Z","steps":["trace[1842841288] 'agreement among raft nodes before linearized reading'  (duration: 186.687829ms)"],"step_count":1}
{"level":"info","ts":"2024-04-24T12:42:52.648173Z","caller":"traceutil/trace.go:171","msg":"trace[905261880] transaction","detail":"{read_only:false; response_revision:46773; number_of_response:1; }","duration":"324.509032ms","start":"2024-04-24T12:42:52.323616Z","end":"2024-04-24T12:42:52.648125Z","steps":["trace[905261880] 'process raft request'  (duration: 206.734435ms)","trace[905261880] 'compare'  (duration: 115.371755ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-24T12:42:52.648437Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-24T12:42:52.323566Z","time spent":"324.7341ms","remote":"127.0.0.1:33552","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":11480,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/pods/default/happy-panda-wordpress-66789d9f64-j87dr\" mod_revision:46759 > success:<request_put:<key:\"/registry/pods/default/happy-panda-wordpress-66789d9f64-j87dr\" value_size:11411 >> failure:<request_range:<key:\"/registry/pods/default/happy-panda-wordpress-66789d9f64-j87dr\" > >"}
{"level":"info","ts":"2024-04-24T12:42:52.96649Z","caller":"traceutil/trace.go:171","msg":"trace[834321726] transaction","detail":"{read_only:false; response_revision:46774; number_of_response:1; }","duration":"300.187384ms","start":"2024-04-24T12:42:52.666228Z","end":"2024-04-24T12:42:52.966416Z","steps":["trace[834321726] 'process raft request'  (duration: 299.90493ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-24T12:42:52.966784Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-24T12:42:52.666169Z","time spent":"300.463605ms","remote":"127.0.0.1:33524","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:46767 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-04-24T12:42:53.000289Z","caller":"traceutil/trace.go:171","msg":"trace[1400727493] transaction","detail":"{read_only:false; response_revision:46775; number_of_response:1; }","duration":"204.20827ms","start":"2024-04-24T12:42:52.796035Z","end":"2024-04-24T12:42:53.000243Z","steps":["trace[1400727493] 'process raft request'  (duration: 203.887277ms)"],"step_count":1}
{"level":"info","ts":"2024-04-24T12:42:53.32015Z","caller":"traceutil/trace.go:171","msg":"trace[360231118] transaction","detail":"{read_only:false; response_revision:46776; number_of_response:1; }","duration":"117.356021ms","start":"2024-04-24T12:42:53.202775Z","end":"2024-04-24T12:42:53.320131Z","steps":["trace[360231118] 'process raft request'  (duration: 117.20048ms)"],"step_count":1}
{"level":"info","ts":"2024-04-24T12:42:53.525495Z","caller":"traceutil/trace.go:171","msg":"trace[1541134081] transaction","detail":"{read_only:false; response_revision:46777; number_of_response:1; }","duration":"198.568018ms","start":"2024-04-24T12:42:53.326889Z","end":"2024-04-24T12:42:53.525457Z","steps":["trace[1541134081] 'process raft request'  (duration: 198.162486ms)"],"step_count":1}
{"level":"info","ts":"2024-04-24T12:42:53.711085Z","caller":"traceutil/trace.go:171","msg":"trace[754913165] transaction","detail":"{read_only:false; number_of_response:1; response_revision:46779; }","duration":"103.178423ms","start":"2024-04-24T12:42:53.60786Z","end":"2024-04-24T12:42:53.711039Z","steps":["trace[754913165] 'process raft request'  (duration: 53.239783ms)","trace[754913165] 'compare'  (duration: 49.440632ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-24T12:42:54.002155Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"172.087101ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028726741255655 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/persistentvolumes/pvc-ba2089ac-577a-4cc2-8230-a7cea00ca804\" mod_revision:46208 > success:<request_put:<key:\"/registry/persistentvolumes/pvc-ba2089ac-577a-4cc2-8230-a7cea00ca804\" value_size:1137 >> failure:<request_range:<key:\"/registry/persistentvolumes/pvc-ba2089ac-577a-4cc2-8230-a7cea00ca804\" > >>","response":"size:18"}
{"level":"info","ts":"2024-04-24T12:42:54.002452Z","caller":"traceutil/trace.go:171","msg":"trace[1300110428] transaction","detail":"{read_only:false; response_revision:46780; number_of_response:1; }","duration":"272.2008ms","start":"2024-04-24T12:42:53.730195Z","end":"2024-04-24T12:42:54.002396Z","steps":["trace[1300110428] 'process raft request'  (duration: 99.71726ms)","trace[1300110428] 'compare'  (duration: 171.8809ms)"],"step_count":2}
{"level":"info","ts":"2024-04-24T12:42:55.054268Z","caller":"traceutil/trace.go:171","msg":"trace[1816351536] transaction","detail":"{read_only:false; response_revision:46781; number_of_response:1; }","duration":"181.165245ms","start":"2024-04-24T12:42:54.873049Z","end":"2024-04-24T12:42:55.054214Z","steps":["trace[1816351536] 'process raft request'  (duration: 180.786712ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-24T12:42:55.317159Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.698984ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028726741255664 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:46774 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2024-04-24T12:42:55.317371Z","caller":"traceutil/trace.go:171","msg":"trace[69802049] linearizableReadLoop","detail":"{readStateIndex:58878; appliedIndex:58877; }","duration":"247.026762ms","start":"2024-04-24T12:42:55.070309Z","end":"2024-04-24T12:42:55.317336Z","steps":["trace[69802049] 'read index received'  (duration: 133.228497ms)","trace[69802049] 'applied index is now lower than readState.Index'  (duration: 113.793888ms)"],"step_count":2}
{"level":"info","ts":"2024-04-24T12:42:55.317445Z","caller":"traceutil/trace.go:171","msg":"trace[468641820] transaction","detail":"{read_only:false; response_revision:46782; number_of_response:1; }","duration":"251.30217ms","start":"2024-04-24T12:42:55.066096Z","end":"2024-04-24T12:42:55.317398Z","steps":["trace[468641820] 'process raft request'  (duration: 137.223463ms)","trace[468641820] 'compare'  (duration: 113.49126ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-24T12:42:55.317595Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"247.347568ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/pv-protection-controller\" ","response":"range_response_count:1 size:215"}
{"level":"info","ts":"2024-04-24T12:42:55.317705Z","caller":"traceutil/trace.go:171","msg":"trace[379613291] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/pv-protection-controller; range_end:; response_count:1; response_revision:46782; }","duration":"247.459108ms","start":"2024-04-24T12:42:55.070211Z","end":"2024-04-24T12:42:55.317671Z","steps":["trace[379613291] 'agreement among raft nodes before linearized reading'  (duration: 247.259264ms)"],"step_count":1}
{"level":"info","ts":"2024-04-24T12:42:56.690524Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"warn","ts":"2024-04-24T12:42:58.027411Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-24T12:42:57.278278Z","time spent":"749.127951ms","remote":"127.0.0.1:33438","response type":"/etcdserverpb.KV/Txn","request count":0,"request size":0,"response count":0,"response size":0,"request content":""}
{"level":"info","ts":"2024-04-24T12:42:58.241241Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-04-24T12:42:58.241453Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-24T12:42:58.241651Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-24T12:42:58.304684Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-24T12:42:58.304786Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-04-24T12:42:58.483828Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-04-24T12:42:59.317697Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-24T12:42:59.391975Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-24T12:42:59.392087Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  14:43:37 up 10:37,  0 users,  load average: 2.38, 2.39, 2.26
Linux minikube 6.5.0-28-generic #29~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr  4 14:39:20 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [098a312badd5] <==
* I0424 12:37:21.663538       1 trace.go:236] Trace[38774602]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (24-Apr-2024 12:37:21.138) (total time: 524ms):
Trace[38774602]: ---"Transaction prepared" 299ms (12:37:21.439)
Trace[38774602]: ---"Txn call completed" 224ms (12:37:21.663)
Trace[38774602]: [524.906398ms] [524.906398ms] END
I0424 12:37:31.738355       1 trace.go:236] Trace[369427856]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (24-Apr-2024 12:37:31.139) (total time: 599ms):
Trace[369427856]: ---"initial value restored" 289ms (12:37:31.428)
Trace[369427856]: ---"Txn call completed" 263ms (12:37:31.738)
Trace[369427856]: [599.241695ms] [599.241695ms] END
I0424 12:38:19.369892       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0424 12:39:16.236376       1 trace.go:236] Trace[1571632451]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:69b4ada1-1b03-4071-a6d1-fa3ae1fadbf4,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (24-Apr-2024 12:39:15.692) (total time: 543ms):
Trace[1571632451]: ["GuaranteedUpdate etcd3" audit-id:69b4ada1-1b03-4071-a6d1-fa3ae1fadbf4,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 543ms (12:39:15.693)
Trace[1571632451]:  ---"Txn call completed" 541ms (12:39:16.236)]
Trace[1571632451]: [543.280181ms] [543.280181ms] END
I0424 12:39:19.370727       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0424 12:40:16.878900       1 trace.go:236] Trace[978037079]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b589c15c-94ff-446d-91b8-c680a096a0c9,client:::1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (24-Apr-2024 12:40:16.179) (total time: 699ms):
Trace[978037079]: ["GuaranteedUpdate etcd3" audit-id:b589c15c-94ff-446d-91b8-c680a096a0c9,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 698ms (12:40:16.179)
Trace[978037079]:  ---"Txn call completed" 696ms (12:40:16.878)]
Trace[978037079]: [699.250098ms] [699.250098ms] END
I0424 12:40:16.880342       1 trace.go:236] Trace[1117126280]: "Update" accept:application/json, */*,audit-id:77746055-cb79-4c21-8c1f-d5e392be83e2,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (24-Apr-2024 12:40:15.483) (total time: 1396ms):
Trace[1117126280]: ["GuaranteedUpdate etcd3" audit-id:77746055-cb79-4c21-8c1f-d5e392be83e2,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1395ms (12:40:15.484)
Trace[1117126280]:  ---"Txn call completed" 1393ms (12:40:16.878)]
Trace[1117126280]: [1.396260535s] [1.396260535s] END
I0424 12:40:19.370741       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0424 12:40:20.884302       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0424 12:41:19.401041       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0424 12:41:41.995864       1 trace.go:236] Trace[205295180]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (24-Apr-2024 12:41:41.154) (total time: 841ms):
Trace[205295180]: ---"initial value restored" 281ms (12:41:41.435)
Trace[205295180]: ---"Transaction prepared" 160ms (12:41:41.596)
Trace[205295180]: ---"Txn call completed" 399ms (12:41:41.995)
Trace[205295180]: [841.169855ms] [841.169855ms] END
I0424 12:42:19.368639       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0424 12:42:45.155058       1 trace.go:236] Trace[1355050843]: "Delete" accept:application/json,audit-id:3819835e-f95e-40a6-b89a-a25cdfba2417,client:192.168.49.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/happy-panda-mariadb,user-agent:Helm/3.14.4,verb:DELETE (24-Apr-2024 12:42:44.045) (total time: 1109ms):
Trace[1355050843]: ---"Object deleted from database" 1108ms (12:42:45.154)
Trace[1355050843]: [1.109357171s] [1.109357171s] END
I0424 12:42:45.221801       1 trace.go:236] Trace[1741635902]: "Delete" accept:application/json,audit-id:1a441e23-94c5-41fb-b0f9-1f42a5210a96,client:192.168.49.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/happy-panda-wordpress,user-agent:Helm/3.14.4,verb:DELETE (24-Apr-2024 12:42:44.045) (total time: 1176ms):
Trace[1741635902]: ---"Object deleted from database" 1175ms (12:42:45.221)
Trace[1741635902]: [1.176055264s] [1.176055264s] END
I0424 12:42:46.207852       1 trace.go:236] Trace[1648572569]: "Update" accept:application/json, */*,audit-id:4e6b7d0d-de76-42db-9bf6-0ac3c8d79c5f,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (24-Apr-2024 12:42:45.676) (total time: 531ms):
Trace[1648572569]: ["GuaranteedUpdate etcd3" audit-id:4e6b7d0d-de76-42db-9bf6-0ac3c8d79c5f,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 530ms (12:42:45.676)
Trace[1648572569]:  ---"Txn call completed" 528ms (12:42:46.207)]
Trace[1648572569]: [531.667249ms] [531.667249ms] END
I0424 12:42:46.208415       1 trace.go:236] Trace[445483119]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a89b234e-7665-4954-aed8-e2c0ec903075,client:192.168.49.2,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/default/replicasets/happy-panda-wordpress-66789d9f64,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:DELETE (24-Apr-2024 12:42:45.670) (total time: 538ms):
Trace[445483119]: ---"Object deleted from database" 537ms (12:42:46.208)
Trace[445483119]: [538.271937ms] [538.271937ms] END
I0424 12:42:46.514744       1 trace.go:236] Trace[1543837410]: "Delete" accept:application/json,audit-id:8eb8a9f8-35d8-4db5-a80e-0e8937a75ffe,client:192.168.49.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/default/configmaps/happy-panda-mariadb,user-agent:Helm/3.14.4,verb:DELETE (24-Apr-2024 12:42:45.757) (total time: 757ms):
Trace[1543837410]: ---"Object deleted from database" 756ms (12:42:46.514)
Trace[1543837410]: [757.062337ms] [757.062337ms] END
I0424 12:42:47.148268       1 trace.go:236] Trace[1192511523]: "Delete" accept:application/json,audit-id:40d3af80-23ab-4b2e-974a-d8b3126170c8,client:192.168.49.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/default/serviceaccounts/happy-panda-wordpress,user-agent:Helm/3.14.4,verb:DELETE (24-Apr-2024 12:42:46.565) (total time: 582ms):
Trace[1192511523]: ---"Object deleted from database" 582ms (12:42:47.147)
Trace[1192511523]: [582.827618ms] [582.827618ms] END
I0424 12:42:47.483055       1 trace.go:236] Trace[98864084]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:020492a5-a066-4ae8-ab8c-6a0d11d96be5,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (24-Apr-2024 12:42:46.885) (total time: 597ms):
Trace[98864084]: ["Create etcd3" audit-id:020492a5-a066-4ae8-ab8c-6a0d11d96be5,key:/events/default/happy-panda-wordpress-66789d9f64-j87dr.17c937dfa5428055,type:*core.Event,resource:events 596ms (12:42:46.886)
Trace[98864084]:  ---"Txn call succeeded" 595ms (12:42:47.482)]
Trace[98864084]: [597.588459ms] [597.588459ms] END
I0424 12:42:47.483788       1 trace.go:236] Trace[676796841]: "Delete" accept:application/json,audit-id:e0690efd-f00e-4f2e-91a4-a0c471c4e438,client:192.168.49.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/default/serviceaccounts/happy-panda-mariadb,user-agent:Helm/3.14.4,verb:DELETE (24-Apr-2024 12:42:46.565) (total time: 918ms):
Trace[676796841]: ---"Object deleted from database" 917ms (12:42:47.483)
Trace[676796841]: [918.188575ms] [918.188575ms] END
I0424 12:42:47.487277       1 trace.go:236] Trace[1011236448]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:48b623a1-9aed-4c08-b492-ac6e29cb0af0,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/default/pods/happy-panda-wordpress-66789d9f64-j87dr,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:GET (24-Apr-2024 12:42:46.885) (total time: 601ms):
Trace[1011236448]: ---"About to write a response" 600ms (12:42:47.486)
Trace[1011236448]: [601.932776ms] [601.932776ms] END

* 
* ==> kube-apiserver [0b5586d895bf] <==
* Trace[2094553301]: [600.998872ms] [600.998872ms] END
I0511 14:34:34.170849       1 trace.go:236] Trace[499472173]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c2d2c894-c082-4805-87dd-3246dfc465bb,client:192.168.49.2,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/ui/deployments/resume/status,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:deployment-controller,verb:PUT (11-May-2024 14:34:33.565) (total time: 605ms):
Trace[499472173]: ["GuaranteedUpdate etcd3" audit-id:c2d2c894-c082-4805-87dd-3246dfc465bb,key:/deployments/ui/resume,type:*apps.Deployment,resource:deployments.apps 604ms (14:34:33.565)
Trace[499472173]:  ---"Txn call completed" 596ms (14:34:34.168)]
Trace[499472173]: [605.134715ms] [605.134715ms] END
I0511 14:35:15.932084       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:35:35.010419       1 trace.go:236] Trace[1625545194]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:66a5d4bb-55fb-4021-ae0b-3274e22ad03c,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/app/events/api-7665c9f64f-4psjb.17ce75d4d6b0ceec,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (11-May-2024 14:35:34.405) (total time: 604ms):
Trace[1625545194]: ["GuaranteedUpdate etcd3" audit-id:66a5d4bb-55fb-4021-ae0b-3274e22ad03c,key:/events/app/api-7665c9f64f-4psjb.17ce75d4d6b0ceec,type:*core.Event,resource:events 604ms (14:35:34.405)
Trace[1625545194]:  ---"initial value restored" 264ms (14:35:34.670)
Trace[1625545194]:  ---"Txn call completed" 335ms (14:35:35.010)]
Trace[1625545194]: ---"Object stored in database" 337ms (14:35:35.010)
Trace[1625545194]: [604.944097ms] [604.944097ms] END
I0511 14:36:15.933118       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:36:16.029152       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:37:15.932107       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:37:51.843202       1 trace.go:236] Trace[1472174430]: "Update" accept:application/json, */*,audit-id:2264e1eb-9649-45cf-b85a-02266db0d611,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-May-2024 14:37:50.632) (total time: 1210ms):
Trace[1472174430]: ["GuaranteedUpdate etcd3" audit-id:2264e1eb-9649-45cf-b85a-02266db0d611,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1210ms (14:37:50.632)
Trace[1472174430]:  ---"Txn call completed" 1207ms (14:37:51.840)]
Trace[1472174430]: [1.210348411s] [1.210348411s] END
I0511 14:37:58.164215       1 trace.go:236] Trace[956869618]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (11-May-2024 14:37:57.433) (total time: 730ms):
Trace[956869618]: ---"Transaction prepared" 303ms (14:37:57.738)
Trace[956869618]: ---"Txn call completed" 425ms (14:37:58.164)
Trace[956869618]: [730.658384ms] [730.658384ms] END
I0511 14:38:16.095298       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:38:57.996900       1 trace.go:236] Trace[2070116976]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (11-May-2024 14:38:57.436) (total time: 560ms):
Trace[2070116976]: ---"Transaction prepared" 147ms (14:38:57.584)
Trace[2070116976]: ---"Txn call completed" 411ms (14:38:57.996)
Trace[2070116976]: [560.08789ms] [560.08789ms] END
I0511 14:39:08.665465       1 trace.go:236] Trace[1601809188]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (11-May-2024 14:39:07.437) (total time: 1227ms):
Trace[1601809188]: ---"initial value restored" 253ms (14:39:07.691)
Trace[1601809188]: ---"Transaction prepared" 347ms (14:39:08.038)
Trace[1601809188]: ---"Txn call completed" 627ms (14:39:08.665)
Trace[1601809188]: [1.227892748s] [1.227892748s] END
I0511 14:39:15.928792       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:40:15.923562       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:40:49.697076       1 trace.go:236] Trace[1934017457]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b8009ed7-6119-467a-b3de-29b1924ca820,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (11-May-2024 14:40:49.188) (total time: 508ms):
Trace[1934017457]: ["GuaranteedUpdate etcd3" audit-id:b8009ed7-6119-467a-b3de-29b1924ca820,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 508ms (14:40:49.188)
Trace[1934017457]:  ---"Txn call completed" 505ms (14:40:49.696)]
Trace[1934017457]: [508.66446ms] [508.66446ms] END
I0511 14:41:15.932811       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:41:16.037032       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:41:58.053198       1 trace.go:236] Trace[1065266630]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (11-May-2024 14:41:57.449) (total time: 603ms):
Trace[1065266630]: ---"initial value restored" 82ms (14:41:57.531)
Trace[1065266630]: ---"Transaction prepared" 211ms (14:41:57.743)
Trace[1065266630]: ---"Txn call completed" 308ms (14:41:58.052)
Trace[1065266630]: [603.459327ms] [603.459327ms] END
I0511 14:42:15.939092       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:43:04.788650       1 trace.go:236] Trace[1342894014]: "Update" accept:application/json, */*,audit-id:5a3206c2-5f8c-48ee-a777-1a5ae7beb1cd,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-May-2024 14:43:04.267) (total time: 521ms):
Trace[1342894014]: ["GuaranteedUpdate etcd3" audit-id:5a3206c2-5f8c-48ee-a777-1a5ae7beb1cd,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 520ms (14:43:04.268)
Trace[1342894014]:  ---"Txn call completed" 517ms (14:43:04.787)]
Trace[1342894014]: [521.149756ms] [521.149756ms] END
I0511 14:43:07.989426       1 trace.go:236] Trace[1087692381]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (11-May-2024 14:43:07.454) (total time: 534ms):
Trace[1087692381]: ---"Transaction prepared" 193ms (14:43:07.650)
Trace[1087692381]: ---"Txn call completed" 339ms (14:43:07.989)
Trace[1087692381]: [534.970889ms] [534.970889ms] END
I0511 14:43:15.926030       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0511 14:43:22.150523       1 trace.go:236] Trace[1076562434]: "Update" accept:application/json, */*,audit-id:b9cd974a-fb05-47f1-84c6-8b958fa79005,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-May-2024 14:43:21.648) (total time: 502ms):
Trace[1076562434]: ["GuaranteedUpdate etcd3" audit-id:b9cd974a-fb05-47f1-84c6-8b958fa79005,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 501ms (14:43:21.649)
Trace[1076562434]:  ---"Txn call completed" 497ms (14:43:22.150)]
Trace[1076562434]: [502.260415ms] [502.260415ms] END

* 
* ==> kube-controller-manager [2b01219b4782] <==
* I0511 14:32:21.310744       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/mongo-7d44888d6f" duration="837.264767ms"
I0511 14:32:21.529238       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/mongo-7d44888d6f" duration="218.380223ms"
I0511 14:32:21.530035       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/mongo-7d44888d6f" duration="114.076¬µs"
I0511 14:32:22.288981       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/mongo-7d44888d6f" duration="250.3¬µs"
I0511 14:32:31.870986       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/mongo-7d44888d6f" duration="273.492649ms"
I0511 14:32:31.871224       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/mongo-7d44888d6f" duration="100.006¬µs"
I0511 14:33:06.955000       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/mongo-7d44888d6f" duration="30.543¬µs"
I0511 14:33:18.429378       1 event.go:307] "Event occurred" object="data/mongo" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mongo-7d44888d6f to 1"
I0511 14:33:18.706883       1 event.go:307] "Event occurred" object="data/mongo-7d44888d6f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mongo-7d44888d6f-rwdth"
I0511 14:33:19.073947       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="645.327651ms"
I0511 14:33:19.332930       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="258.842061ms"
I0511 14:33:19.333606       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="550.36¬µs"
I0511 14:33:19.334184       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="481.874¬µs"
I0511 14:33:19.574467       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="122.16¬µs"
I0511 14:33:26.893734       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="93.181422ms"
I0511 14:33:26.893866       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="83.643¬µs"
I0511 14:33:32.188775       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="22.311¬µs"
I0511 14:33:39.569158       1 event.go:307] "Event occurred" object="data/mongo" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mongo-7d44888d6f to 1"
I0511 14:33:39.846759       1 event.go:307] "Event occurred" object="data/mongo-7d44888d6f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mongo-7d44888d6f-7x54p"
I0511 14:33:40.099891       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="531.318388ms"
I0511 14:33:40.663508       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="563.443495ms"
I0511 14:33:40.663884       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="172.629¬µs"
I0511 14:33:47.270794       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="204.609209ms"
I0511 14:33:47.271594       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="data/mongo-7d44888d6f" duration="93.716¬µs"
I0511 14:34:15.775582       1 event.go:307] "Event occurred" object="app/api" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set api-7665c9f64f to 1"
I0511 14:34:16.072345       1 event.go:307] "Event occurred" object="app/api-7665c9f64f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: api-7665c9f64f-4psjb"
I0511 14:34:16.331567       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="556.995294ms"
I0511 14:34:16.396247       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="63.892142ms"
I0511 14:34:16.396811       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="219.263¬µs"
I0511 14:34:16.605342       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="157.133¬µs"
I0511 14:34:25.358232       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="203.985¬µs"
I0511 14:34:33.225962       1 event.go:307] "Event occurred" object="ui/resume" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set resume-66d5fbff68 to 1"
I0511 14:34:33.561600       1 event.go:307] "Event occurred" object="ui/resume-66d5fbff68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: resume-66d5fbff68-8srpc"
I0511 14:34:34.171844       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="946.089677ms"
I0511 14:34:34.599357       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="427.371426ms"
I0511 14:34:34.599610       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="137.511¬µs"
I0511 14:34:34.669170       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="203.35¬µs"
I0511 14:34:37.591106       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="138.86¬µs"
I0511 14:34:55.944149       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="609.384¬µs"
I0511 14:35:06.580790       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="201.461¬µs"
I0511 14:35:07.018055       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="135.593¬µs"
I0511 14:35:18.753683       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="135.713¬µs"
I0511 14:35:21.552991       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="178.754¬µs"
I0511 14:35:34.672972       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="145.475¬µs"
I0511 14:35:35.524397       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="145.595¬µs"
I0511 14:35:45.660807       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="117.906¬µs"
I0511 14:35:51.670788       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="156.388¬µs"
I0511 14:36:04.628219       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="261.368¬µs"
I0511 14:36:22.427469       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="203.991¬µs"
I0511 14:36:35.635151       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="188.349¬µs"
I0511 14:36:48.655073       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="222.78¬µs"
I0511 14:37:01.755406       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="231.774¬µs"
I0511 14:37:59.566184       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="212.313¬µs"
I0511 14:38:13.860853       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="228.782¬µs"
I0511 14:38:14.440615       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="377.236¬µs"
I0511 14:38:25.405295       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="88.391¬µs"
I0511 14:40:48.518517       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="260.157¬µs"
I0511 14:41:00.531302       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="app/api-7665c9f64f" duration="381.957¬µs"
I0511 14:41:08.682017       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="235.902¬µs"
I0511 14:41:20.517198       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ui/resume-66d5fbff68" duration="213.725¬µs"

* 
* ==> kube-controller-manager [2c22a978dea0] <==
* I0424 12:31:06.989794       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="1.124631672s"
I0424 12:31:06.989794       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="1.123351645s"
I0424 12:31:07.100378       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="194.482¬µs"
I0424 12:31:07.100643       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="137.445¬µs"
I0424 12:31:08.401379       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="2.681571452s"
I0424 12:31:08.401722       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="2.810950107s"
I0424 12:31:08.402154       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="326.873¬µs"
I0424 12:31:08.405561       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="160.11¬µs"
I0424 12:31:33.936147       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="67.18¬µs"
E0424 12:31:36.102359       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0424 12:31:36.309275       1 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0424 12:31:36.814767       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="695.074906ms"
I0424 12:31:36.815955       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="40.244¬µs"
I0424 12:31:38.547231       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="97.447¬µs"
I0424 12:31:38.547750       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="623.22018ms"
I0424 12:31:38.547981       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="174.273¬µs"
E0424 12:32:06.693616       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0424 12:32:06.910408       1 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0424 12:32:08.497342       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="464.117625ms"
I0424 12:32:08.498290       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="76.755¬µs"
I0424 12:32:14.046235       1 event.go:307] "Event occurred" object="default/happy-panda-wordpress" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0424 12:32:14.046617       1 event.go:307] "Event occurred" object="default/happy-panda-wordpress" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0424 12:32:15.058934       1 event.go:307] "Event occurred" object="default/happy-panda-wordpress" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set happy-panda-wordpress-66789d9f64 to 1"
I0424 12:32:15.498901       1 event.go:307] "Event occurred" object="default/happy-panda-wordpress-66789d9f64" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: happy-panda-wordpress-66789d9f64-j87dr"
I0424 12:32:15.936081       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="877.803811ms"
I0424 12:32:16.477995       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="541.495568ms"
I0424 12:32:16.478371       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="240.5¬µs"
I0424 12:32:16.894791       1 event.go:307] "Event occurred" object="default/happy-panda-mariadb" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim data-happy-panda-mariadb-0 Pod happy-panda-mariadb-0 in StatefulSet happy-panda-mariadb success"
I0424 12:32:17.156393       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="268.088¬µs"
I0424 12:32:17.226078       1 event.go:307] "Event occurred" object="default/data-happy-panda-mariadb-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0424 12:32:17.228150       1 event.go:307] "Event occurred" object="default/happy-panda-mariadb" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod happy-panda-mariadb-0 in StatefulSet happy-panda-mariadb successful"
I0424 12:32:21.674141       1 event.go:307] "Event occurred" object="default/data-happy-panda-mariadb-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0424 12:32:21.674256       1 event.go:307] "Event occurred" object="default/happy-panda-wordpress" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0424 12:32:26.093416       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="720.023251ms"
I0424 12:32:26.093869       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="200.39¬µs"
I0424 12:32:27.377052       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="215.783¬µs"
I0424 12:32:31.600310       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="175.353¬µs"
I0424 12:32:36.674890       1 event.go:307] "Event occurred" object="default/data-happy-panda-mariadb-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0424 12:32:36.674954       1 event.go:307] "Event occurred" object="default/happy-panda-wordpress" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
E0424 12:32:36.704684       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0424 12:32:36.938489       1 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0424 12:32:46.431789       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="265.965134ms"
I0424 12:32:46.432169       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="122.996¬µs"
I0424 12:32:51.675715       1 event.go:307] "Event occurred" object="default/data-happy-panda-mariadb-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0424 12:32:51.675817       1 event.go:307] "Event occurred" object="default/happy-panda-wordpress" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0424 12:33:04.416209       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="472.591¬µs"
I0424 12:33:04.761425       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="518.751¬µs"
E0424 12:33:06.719986       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0424 12:33:06.964315       1 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E0424 12:33:36.734835       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0424 12:33:36.986907       1 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0424 12:33:38.315832       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="211.548948ms"
I0424 12:33:38.317022       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="161.551¬µs"
I0424 12:35:53.024367       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="244.843¬µs"
I0424 12:36:35.057878       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="311.662¬µs"
I0424 12:36:39.030248       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="157.453¬µs"
I0424 12:38:08.764714       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="384.143235ms"
I0424 12:38:08.765684       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="224.237¬µs"
I0424 12:42:45.404920       1 stateful_set.go:458] "StatefulSet has been deleted" key="default/happy-panda-mariadb"
I0424 12:42:46.335425       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/happy-panda-wordpress-66789d9f64" duration="27.432¬µs"

* 
* ==> kube-proxy [4fdfaf6a2e3d] <==
* I0424 12:31:53.515583       1 server_others.go:69] "Using iptables proxy"
I0424 12:31:54.420961       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0424 12:31:55.975949       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0424 12:31:55.984591       1 server_others.go:152] "Using iptables Proxier"
I0424 12:31:55.984703       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0424 12:31:55.984749       1 server_others.go:438] "Defaulting to no-op detect-local"
I0424 12:31:56.023696       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0424 12:31:56.024616       1 server.go:846] "Version info" version="v1.28.3"
I0424 12:31:56.024677       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0424 12:31:56.066195       1 config.go:97] "Starting endpoint slice config controller"
I0424 12:31:56.066449       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0424 12:31:56.066217       1 config.go:188] "Starting service config controller"
I0424 12:31:56.067144       1 shared_informer.go:311] Waiting for caches to sync for service config
I0424 12:31:56.066312       1 config.go:315] "Starting node config controller"
I0424 12:31:56.067398       1 shared_informer.go:311] Waiting for caches to sync for node config
I0424 12:31:56.172315       1 shared_informer.go:318] Caches are synced for node config
I0424 12:31:56.174459       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0424 12:31:56.467861       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [ee7119a6df24] <==
* I0511 14:22:40.291433       1 server_others.go:69] "Using iptables proxy"
I0511 14:22:41.074318       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0511 14:22:42.119115       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0511 14:22:42.129950       1 server_others.go:152] "Using iptables Proxier"
I0511 14:22:42.130075       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0511 14:22:42.130119       1 server_others.go:438] "Defaulting to no-op detect-local"
I0511 14:22:42.130207       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0511 14:22:42.131302       1 server.go:846] "Version info" version="v1.28.3"
I0511 14:22:42.131358       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0511 14:22:42.161326       1 config.go:188] "Starting service config controller"
I0511 14:22:42.161425       1 shared_informer.go:311] Waiting for caches to sync for service config
I0511 14:22:42.161342       1 config.go:97] "Starting endpoint slice config controller"
I0511 14:22:42.161609       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0511 14:22:42.161689       1 config.go:315] "Starting node config controller"
I0511 14:22:42.161711       1 shared_informer.go:311] Waiting for caches to sync for node config
I0511 14:22:42.262638       1 shared_informer.go:318] Caches are synced for node config
I0511 14:22:42.262743       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0511 14:22:42.262771       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [ae7f875ad708] <==
* E0511 14:21:10.971554       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:11.783594       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:11.783783       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:11.786668       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:11.786868       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:11.885126       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:11.885280       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:11.937298       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:11.937327       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.038877       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.038961       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.064009       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.064145       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.089426       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.089583       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.131856       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.131988       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.133479       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.133609       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.247929       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.247965       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.273146       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.273181       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.281738       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.281805       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.503547       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.503585       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.507987       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.508023       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:12.516785       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0511 14:21:12.516831       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0511 14:21:16.004508       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0511 14:21:16.004561       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0511 14:21:16.004660       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0511 14:21:16.004681       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0511 14:21:16.004802       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0511 14:21:16.004822       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0511 14:21:16.004965       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0511 14:21:16.004986       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0511 14:21:16.005048       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0511 14:21:16.005066       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0511 14:21:16.005118       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0511 14:21:16.005139       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0511 14:21:16.005212       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0511 14:21:16.005231       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0511 14:21:16.005281       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0511 14:21:16.005966       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0511 14:21:16.006314       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0511 14:21:16.007115       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0511 14:21:16.007376       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0511 14:21:16.007496       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0511 14:21:16.007669       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0511 14:21:16.007776       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0511 14:21:16.008527       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0511 14:21:16.008669       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0511 14:21:16.009016       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0511 14:21:16.009118       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0511 14:21:16.014833       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0511 14:21:16.018957       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
I0511 14:21:16.110500       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [cf6145a6f06e] <==
* W0424 12:30:05.765400       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:05.765537       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:05.788325       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:05.788438       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:05.992974       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:05.993118       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:06.044326       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:06.044467       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:06.058684       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:06.058822       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:06.320454       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:06.320657       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:06.575365       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:06.575491       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:08.977537       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:08.977658       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:08.982882       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:08.983017       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:09.733234       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:09.733356       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:09.766770       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:09.766909       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:10.300507       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:10.300601       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:10.356503       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:10.357182       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:10.425282       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:10.425405       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:10.464201       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:10.464353       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:10.728513       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:10.728651       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:10.838943       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:10.839044       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:11.231542       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:11.231624       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:11.295411       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:11.295549       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:11.468396       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:11.468520       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:11.744684       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0424 12:30:11.744820       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0424 12:30:20.230354       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0424 12:30:20.230698       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0424 12:30:20.230353       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0424 12:30:20.231270       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0424 12:30:20.230363       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0424 12:30:20.231565       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0424 12:30:20.230453       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0424 12:30:20.231731       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0424 12:30:20.230487       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0424 12:30:20.231892       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0424 12:30:20.230462       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0424 12:30:20.232121       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0424 12:30:20.230539       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0424 12:30:20.232252       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
I0424 12:30:23.679829       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0424 12:42:56.692412       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0424 12:42:57.162868       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0424 12:42:57.699723       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259

* 
* ==> kubelet <==
* May 11 14:39:52 minikube kubelet[1172]: E0511 14:39:52.368602    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:39:54 minikube kubelet[1172]: E0511 14:39:54.361996    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:39:54 minikube kubelet[1172]: E0511 14:39:54.361998    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:40:06 minikube kubelet[1172]: E0511 14:40:06.366140    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:40:06 minikube kubelet[1172]: E0511 14:40:06.366748    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:40:07 minikube kubelet[1172]: E0511 14:40:07.358111    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:40:19 minikube kubelet[1172]: E0511 14:40:19.358394    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:40:19 minikube kubelet[1172]: E0511 14:40:19.358410    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:40:22 minikube kubelet[1172]: E0511 14:40:22.363456    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:40:32 minikube kubelet[1172]: E0511 14:40:32.361289    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:40:37 minikube kubelet[1172]: E0511 14:40:37.086567    1172 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for api, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="api:v1"
May 11 14:40:37 minikube kubelet[1172]: E0511 14:40:37.086684    1172 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for api, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="api:v1"
May 11 14:40:37 minikube kubelet[1172]: E0511 14:40:37.087020    1172 kuberuntime_manager.go:1256] container &Container{Name:api,Image:api:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9sbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-7665c9f64f-4psjb_app(528ee7c4-31a6-456b-88ae-c53b2500d6f8): ErrImagePull: Error response from daemon: pull access denied for api, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 11 14:40:37 minikube kubelet[1172]: E0511 14:40:37.087192    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ErrImagePull: \"Error response from daemon: pull access denied for api, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:40:37 minikube kubelet[1172]: E0511 14:40:37.358760    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:40:45 minikube kubelet[1172]: E0511 14:40:45.362647    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:40:48 minikube kubelet[1172]: E0511 14:40:48.359235    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:40:53 minikube kubelet[1172]: E0511 14:40:53.905795    1172 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for resume, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resume:v1"
May 11 14:40:53 minikube kubelet[1172]: E0511 14:40:53.905927    1172 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for resume, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resume:v1"
May 11 14:40:53 minikube kubelet[1172]: E0511 14:40:53.906258    1172 kuberuntime_manager.go:1256] container &Container{Name:myapp,Image:resume:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8181,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58ncz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod resume-66d5fbff68-8srpc_ui(a8918d70-5ece-419d-9f5d-2a83f74e9dcd): ErrImagePull: Error response from daemon: pull access denied for resume, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 11 14:40:53 minikube kubelet[1172]: E0511 14:40:53.906420    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ErrImagePull: \"Error response from daemon: pull access denied for resume, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:40:59 minikube kubelet[1172]: E0511 14:40:59.362082    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:41:00 minikube kubelet[1172]: E0511 14:41:00.383005    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:41:08 minikube kubelet[1172]: E0511 14:41:08.359271    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:41:12 minikube kubelet[1172]: E0511 14:41:12.365835    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:41:13 minikube kubelet[1172]: E0511 14:41:13.358529    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:41:20 minikube kubelet[1172]: E0511 14:41:20.363048    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:41:25 minikube kubelet[1172]: E0511 14:41:25.362861    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:41:27 minikube kubelet[1172]: E0511 14:41:27.361992    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:41:35 minikube kubelet[1172]: E0511 14:41:35.360010    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:41:38 minikube kubelet[1172]: E0511 14:41:38.363816    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:41:39 minikube kubelet[1172]: E0511 14:41:39.362200    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:41:47 minikube kubelet[1172]: E0511 14:41:47.362460    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:41:52 minikube kubelet[1172]: E0511 14:41:52.364862    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:41:54 minikube kubelet[1172]: E0511 14:41:54.362729    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:41:58 minikube kubelet[1172]: E0511 14:41:58.373165    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:42:05 minikube kubelet[1172]: E0511 14:42:05.362495    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:42:08 minikube kubelet[1172]: E0511 14:42:08.363914    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:42:09 minikube kubelet[1172]: E0511 14:42:09.362847    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:42:19 minikube kubelet[1172]: E0511 14:42:19.363263    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:42:23 minikube kubelet[1172]: E0511 14:42:23.363610    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:42:24 minikube kubelet[1172]: E0511 14:42:24.364660    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:42:34 minikube kubelet[1172]: E0511 14:42:34.363915    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:42:35 minikube kubelet[1172]: E0511 14:42:35.360319    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:42:35 minikube kubelet[1172]: E0511 14:42:35.361097    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:42:48 minikube kubelet[1172]: E0511 14:42:48.363089    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:42:49 minikube kubelet[1172]: E0511 14:42:49.361979    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:42:50 minikube kubelet[1172]: E0511 14:42:50.362762    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:43:02 minikube kubelet[1172]: E0511 14:43:02.364828    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:43:02 minikube kubelet[1172]: E0511 14:43:02.364993    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:43:03 minikube kubelet[1172]: E0511 14:43:03.362310    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:43:14 minikube kubelet[1172]: E0511 14:43:14.361032    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:43:14 minikube kubelet[1172]: E0511 14:43:14.361145    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:43:17 minikube kubelet[1172]: E0511 14:43:17.362798    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:43:27 minikube kubelet[1172]: E0511 14:43:27.362917    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:43:28 minikube kubelet[1172]: E0511 14:43:28.362621    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:43:29 minikube kubelet[1172]: E0511 14:43:29.363178    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"
May 11 14:43:40 minikube kubelet[1172]: E0511 14:43:40.362957    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ping-google\" with ImagePullBackOff: \"Back-off pulling image \\\"ping-google:v1\\\"\"" pod="exp/ping-google" podUID="5986f228-3be9-4497-a4cf-5b472d7b6219"
May 11 14:43:40 minikube kubelet[1172]: E0511 14:43:40.363511    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with ImagePullBackOff: \"Back-off pulling image \\\"api:v1\\\"\"" pod="app/api-7665c9f64f-4psjb" podUID="528ee7c4-31a6-456b-88ae-c53b2500d6f8"
May 11 14:43:44 minikube kubelet[1172]: E0511 14:43:44.366245    1172 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"resume:v1\\\"\"" pod="ui/resume-66d5fbff68-8srpc" podUID="a8918d70-5ece-419d-9f5d-2a83f74e9dcd"

* 
* ==> kubernetes-dashboard [8c9db876abca] <==
* 2024/05/11 14:22:35 Using namespace: kubernetes-dashboard
2024/05/11 14:22:35 Using in-cluster config to connect to apiserver
2024/05/11 14:22:35 Using secret token for csrf signing
2024/05/11 14:22:35 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/05/11 14:22:35 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00061fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000276100)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf

* 
* ==> kubernetes-dashboard [90ab0497f40c] <==
* 2024/05/11 14:23:27 Starting overwatch
2024/05/11 14:23:27 Using namespace: kubernetes-dashboard
2024/05/11 14:23:27 Using in-cluster config to connect to apiserver
2024/05/11 14:23:27 Using secret token for csrf signing
2024/05/11 14:23:27 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/05/11 14:23:27 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/05/11 14:23:28 Successful initial request to the apiserver, version: v1.28.3
2024/05/11 14:23:28 Generating JWE encryption key
2024/05/11 14:23:28 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/05/11 14:23:28 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/05/11 14:23:28 Initializing JWE encryption key from synchronized object
2024/05/11 14:23:28 Creating in-cluster Sidecar client
2024/05/11 14:23:28 Successful request to sidecar
2024/05/11 14:23:28 Serving insecurely on HTTP port: 9090

